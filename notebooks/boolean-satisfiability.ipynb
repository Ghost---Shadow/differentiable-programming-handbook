{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable Boolean Satisfiability\n",
    "\n",
    "This notebook demonstrates a differentiable implementation of the [Boolean Satisfiability Problem](https://en.wikipedia.org/wiki/Boolean_satisfiability_problem) (SAT), encoded in [Conjunctive Normal Form](https://en.wikipedia.org/wiki/Conjunctive_normal_form) (CNF).\n",
    "\n",
    "## Key Concept\n",
    "\n",
    "The SAT problem asks: given a boolean formula, is there an assignment of true/false values to variables that makes the formula evaluate to true? Traditionally, this is a discrete optimization problem, but by using continuous relaxations, we can apply gradient-based optimization.\n",
    "\n",
    "**Encoding Convention:**\n",
    "- $True$ is encoded as floating point $1.0$  \n",
    "- $False$ is encoded as floating point $0.0$\n",
    "\n",
    "This allows us to use standard neural network optimization techniques to find satisfying assignments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import the necessary libraries and verify our TensorFlow setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices(\"GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable Logic Gates\n",
    "\n",
    "### Differentiable AND Gate\n",
    "\n",
    "Using floating point multiplication as an approximation for the $AND$ gate. This works perfectly for our boolean encoding:\n",
    "\n",
    "$$0.0 \\times 1.0 = 0.0 \\quad (\\text{False AND True} = \\text{False})$$\n",
    "$$1.0 \\times 1.0 = 1.0 \\quad (\\text{True AND True} = \\text{True})$$\n",
    "\n",
    "**Key insight:** Multiplication naturally implements AND logic when inputs are constrained to $\\{0,1\\}$, and the gradients provide useful optimization signals even for intermediate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/tensorflow/tensorflow/issues/29781#issuecomment-542496509\n",
    "def compute_hessian(f, *x):\n",
    "    grad_grads = []\n",
    "    with tf.GradientTape(persistent=True) as hess_tape:\n",
    "        with tf.GradientTape() as grad_tape:\n",
    "            y = f(*x)\n",
    "        grad = grad_tape.gradient(y, x)\n",
    "        for g in grad:\n",
    "            grad_grads += list(hess_tape.gradient(g, x))\n",
    "    hessian = tf.stack(grad_grads)\n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiable NOT Gate\n",
    "\n",
    "Since valid boolean values are constrained between $0.0$ and $1.0$, we can implement negation by subtracting from $1.0$:\n",
    "\n",
    "$$1.0 - 0.0 = 1.0 \\quad (\\text{NOT False} = \\text{True})$$\n",
    "$$1.0 - 1.0 = 0.0 \\quad (\\text{NOT True} = \\text{False})$$\n",
    "\n",
    "**Note:** The gradient is always $-1$, which provides a clear optimization signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 0. 0. 0.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([1. 1. 0. 0.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([1. 0. 1. 0.], shape=(4,), dtype=float32)\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "tf.Tensor(\n",
      "[[0. 0. 0. 0.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [0. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def gate_and(x, y):\n",
    "    return x * y\n",
    "\n",
    "\n",
    "x = tf.Variable([1, 0, 1, 0], dtype=tf.float32)\n",
    "y = tf.Variable([1, 1, 0, 0], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = gate_and(x, y)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, x))\n",
    "print(tape.gradient(z, y))\n",
    "print(compute_hessian(gate_and, x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiable OR Gate\n",
    "\n",
    "We compose the OR gate using [De Morgan's Law](https://en.wikipedia.org/wiki/De_Morgan's_laws):\n",
    "\n",
    "$$ X \\lor Y = \\overline{\\overline{X} \\land \\overline{Y}} $$\n",
    "\n",
    "This is elegant because it reuses our existing AND and NOT implementations, ensuring consistency across all logic operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 1. 0. 1.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([-1. -1. -1. -1.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def gate_not(x):\n",
    "    return 1 - x\n",
    "\n",
    "\n",
    "x = tf.Variable([1, 0, 1, 0], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = gate_not(x)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiable XOR Gate\n",
    "\n",
    "We implement XOR from its definition as \"exclusive or\":\n",
    "\n",
    "$$ X \\oplus Y = (\\overline{X} \\land Y) \\lor (X \\land \\overline{Y}) $$\n",
    "\n",
    "XOR is true when exactly one input is true. This gate will be crucial for our SAT solver implementation, as it helps us determine when variables match their expected polarities in clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 1. 1. 0.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([-0. -0.  1.  1.], shape=(4,), dtype=float32)\n",
      "tf.Tensor([-0.  1. -0.  1.], shape=(4,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def gate_or(x, y):\n",
    "    not_x = gate_not(x)\n",
    "    not_y = gate_not(y)\n",
    "\n",
    "    x_not_y_not = gate_and(not_x, not_y)\n",
    "    x_or_y = gate_not(x_not_y_not)\n",
    "\n",
    "    return x_or_y\n",
    "\n",
    "\n",
    "x = tf.Variable([1, 0, 1, 0], dtype=tf.float32)\n",
    "y = tf.Variable([1, 1, 0, 0], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = gate_or(x, y)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, x))\n",
    "print(tape.gradient(z, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAT Problem Encoding\n",
    "\n",
    "### Candidate Encoding\n",
    "\n",
    "A **candidate** represents a potential solution - an assignment of truth values to variables. For example, if we have variables $X_1, X_2, X_3$, then the assignment $X_1 = False$, $X_2 = True$, $X_3 = False$ is encoded as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Candidate} = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 1.0 & 0.0 \\\\\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "### Circuit Encoding\n",
    "\n",
    "The boolean formula in CNF is encoded as a 2D tensor where each row represents a clause (disjunction) and each column represents a variable. The value indicates whether the variable appears negated in that clause.\n",
    "\n",
    "For example, the formula:\n",
    "$$\n",
    "(X_1 \\lor X_2 \\lor \\overline{X_3}) \\land\n",
    "(X_1 \\lor \\overline{X_2} \\lor X_3) \\land \n",
    "(\\overline{X_1} \\lor X_2 \\lor X_3)\n",
    "$$\n",
    "\n",
    "is encoded as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Circuit} = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & 1.0 \\\\  \\text{  ← } (X_1 \\lor X_2 \\lor \\overline{X_3}) \\\\\n",
    "0.0 & 1.0 & 0.0 \\\\  \\text{  ← } (X_1 \\lor \\overline{X_2} \\lor X_3) \\\\\n",
    "1.0 & 0.0 & 0.0     \\text{  ← } (\\overline{X_1} \\lor X_2 \\lor X_3)\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "where $1.0$ indicates the variable appears negated in that clause, $0.0$ indicates it appears positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient GPU Algorithm\n",
    "\n",
    "To maximize GPU utilization, we use a parallel evaluation strategy:\n",
    "\n",
    "**Step 1: Broadcasting**  \n",
    "First, we broadcast/tile the candidate to match the circuit dimensions:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Candidate} = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 1.0 & 0.0 \\\\\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "becomes (repeated for each clause):\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Candidate}_{broadcasted} = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 1.0 & 0.0 & 0.0 & 1.0 & 0.0 & 0.0 & 1.0 & 0.0 \\\\\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "**Step 2: Flattening**  \n",
    "We flatten the circuit encoding:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{Circuit}_{flat} = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & 1.0 &\n",
    "0.0 & 1.0 & 0.0 &\n",
    "1.0 & 0.0 & 0.0\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "**Step 3: XOR Operation**  \n",
    "The XOR between candidate and circuit determines variable polarities. This efficiently computes whether each variable-clause pair contributes positively to satisfaction:\n",
    "\n",
    "- XOR$(0,0) = 0$: Variable is False and appears positive → contributes False  \n",
    "- XOR$(0,1) = 1$: Variable is False and appears negated → contributes True\n",
    "- XOR$(1,0) = 1$: Variable is True and appears positive → contributes True  \n",
    "- XOR$(1,1) = 0$: Variable is True and appears negated → contributes False\n",
    "\n",
    "**Step 4: Parallel Reduction**  \n",
    "Finally, we perform parallel reductions:\n",
    "1. OR within each clause (any True literal satisfies the clause)\n",
    "2. AND across all clauses (all clauses must be satisfied)\n",
    "\n",
    "This approach leverages GPU parallelism for efficient SAT evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit Graph\n",
    "\n",
    "### Candidate Encoding\n",
    "\n",
    "Candidate is the input to the circuit. The satisfiability problem is considered solved when we find a candidate that makes the circuit output $True$.\n",
    "\n",
    "Candidate $X_1 = 0$, $X_2 = 1$ and $X_3 = 0$ is encoded as tensor.\n",
    "\n",
    "\\begin{equation*}\n",
    "Candidate = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 1.0 & 0.0 \\\\\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "### Circuit Encoding\n",
    "\n",
    "Circuit is encoded as a 2D tensor. e.g.\n",
    "\n",
    "$$\n",
    "(X_1 \\lor X_2 \\lor \\overline{X_3}) \\land\n",
    "(X_1 \\lor \\overline{X_2} \\lor X_3) \\land \n",
    "(\\overline{X_1} \\lor X_2 \\lor X_3)\n",
    "$$\n",
    "\n",
    "is encoded as\n",
    "\n",
    "\\begin{equation*}\n",
    "Circuit = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & 1.0 \\\\\n",
    "0.0 & 1.0 & 0.0 \\\\\n",
    "1.0 & 0.0 & 0.0\n",
    "\\end{bmatrix}\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint/Boundary Loss Function\n",
    "\n",
    "Since we're working with continuous values but need binary solutions, we need a constraint loss that pushes values toward $0.0$ or $1.0$. \n",
    "\n",
    "This loss function has the following properties:\n",
    "- **Minimum at 0 and 1**: $f(0) = f(1) = 0$ \n",
    "- **Maximum at 0.5**: $f(0.5) = 0.0625$\n",
    "- **Steep penalties**: Values far from $\\{0,1\\}$ incur large penalties\n",
    "- **Smooth gradients**: Provides optimization signals throughout $[0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "To make the best use of GPU we follow this method.\n",
    "\n",
    "First we broadcast/tile the candidate to match the dimensions of the problem.\n",
    "\n",
    "\\begin{equation*}\n",
    "Candidate = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 1.0 & 0.0 \\\\\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "becomes\n",
    "\n",
    "\\begin{equation*}\n",
    "Candidate_{broadcasted} = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 1.0 & 0.0 & 0.0 & 1.0 & 0.0 & 0.0 & 1.0 & 0.0 \\\\\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "Next we flatten the circuit\n",
    "\n",
    "\\begin{equation*}\n",
    "Circuit = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 0.0 & 1.0 &\n",
    "0.0 & 1.0 & 0.0 &\n",
    "1.0 & 0.0 & 0.0\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "The XOR of the broadcasted candidate and the flat circuit gives us the activations at each element. e.g.\n",
    "\n",
    "$$\n",
    "(X_1 \\lor X_2 \\lor \\overline{X_3}) \\land\n",
    "(X_1 \\lor \\overline{X_2} \\lor X_3) \\land \n",
    "(\\overline{X_1} \\lor X_2 \\lor X_3)\n",
    "$$\n",
    "$$=$$\n",
    "$$\n",
    "(0.0 \\lor 1.0 \\lor 1.0) \\land\n",
    "(0.0 \\lor 0.0 \\lor 0.0) \\land \n",
    "(1.0 \\lor 1.0 \\lor 0.0)\n",
    "$$\n",
    "\n",
    "We shall now reshape the activations into 2D for the sake of clarity\n",
    "\n",
    "\\begin{equation*}\n",
    "Activations = \n",
    "\\begin{bmatrix}\n",
    "0.0 & 1.0 & 1.0 \\\\\n",
    "0.0 & 0.0 & 0.0 \\\\\n",
    "1.0 & 1.0 & 0.0\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "Now we parallel reduce the $\\lor$ (OR) operations. We get\n",
    "\n",
    "\\begin{equation*}\n",
    "Activations = \n",
    "\\begin{bmatrix}\n",
    "1.0 \\\\\n",
    "0.0 \\\\\n",
    "1.0\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "On reducing the $\\land$ (AND) operations, we get the output of the circuit\n",
    "\n",
    "\\begin{equation*}\n",
    "Activations = \n",
    "\\begin{bmatrix}\n",
    "0.0\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "Since, the whole thing is differentiable end to end. We can optimize using SGD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor([ 1. -1.  1.], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.  0.  0.]\n",
      " [ 1. -1.  1.]\n",
      " [ 0.  0.  0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def circuit_forward_pass(candidate, problem_encoding):\n",
    "    disjunction_count = tf.shape(problem_encoding)[0]\n",
    "\n",
    "    broadcasted_candidate = tf.tile(candidate, [disjunction_count])\n",
    "    reshaped_problem = tf.reshape(problem_encoding, [-1])\n",
    "\n",
    "    resolved_values = gate_xor(broadcasted_candidate, reshaped_problem)\n",
    "    reshaped_values = tf.reshape(resolved_values, [-1, disjunction_count])\n",
    "    reshaped_values = tf.transpose(reshaped_values, [1, 0])\n",
    "\n",
    "    conjunctions = tf.scan(gate_or, reshaped_values)[-1]\n",
    "    conjunctions = tf.reshape(conjunctions, [-1, 1])\n",
    "    disjunctions = tf.scan(gate_and, conjunctions)[-1][0]\n",
    "\n",
    "    return disjunctions\n",
    "\n",
    "\n",
    "candidate = tf.Variable([0, 1, 0], dtype=tf.float32)\n",
    "problem_encoding = tf.Variable(\n",
    "    [\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "    ],\n",
    "    dtype=tf.float32,\n",
    ")\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    z = circuit_forward_pass(candidate, problem_encoding)\n",
    "\n",
    "print(z)\n",
    "print(tape.gradient(z, candidate))\n",
    "print(tape.gradient(z, problem_encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Optimization Loss\n",
    "\n",
    "Our total loss function combines two objectives:\n",
    "\n",
    "1. **Satisfiability Loss**: Drives the circuit output toward $1.0$ (satisfied)\n",
    "2. **Constraint Loss**: Pushes variable assignments toward binary values $\\{0,1\\}$\n",
    "\n",
    "This multi-objective approach finds solutions that both satisfy the boolean formula and have crisp binary assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5625, 0.0, 0.03515625, 0.0625, 0.03515625, 0.0, 0.5625]\n",
      "[-3.0, 0.0, 0.1875, 0.0, -0.1875, 0.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def constraint_loss_fn(candidate):\n",
    "    x = candidate\n",
    "    a = x**2\n",
    "    b = (x - 1) ** 2\n",
    "\n",
    "    return a * b\n",
    "\n",
    "\n",
    "candidate = tf.Variable([-0.5, 0, 0.25, 0.5, 0.75, 1, 1.5], dtype=tf.float32)\n",
    "with tf.GradientTape() as tape:\n",
    "    z = constraint_loss_fn(candidate)\n",
    "    print(list(z.numpy()))\n",
    "    print(list(tape.gradient(z, candidate).numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Optimization\n",
    "\n",
    "Now we can train our differentiable SAT solver! We start with an initial guess for the variable assignments and use Adam optimizer to find a satisfying assignment.\n",
    "\n",
    "The training process shows:\n",
    "- **Loss decreasing**: The solver is finding better assignments\n",
    "- **Variables converging**: Values approach binary $\\{0,1\\}$ solutions  \n",
    "- **Circuit satisfaction**: Output approaches $1.0$ (fully satisfied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization loss\n",
    "Loss to make sure that the output is equal to 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "Let's verify our solution! The optimizer found $X_1 = 1.0$, $X_2 = 1.0$, $X_3 = 1.0$ as a satisfying assignment.\n",
    "\n",
    "**Manual verification:**\n",
    "- Clause 1: $(1 \\lor 1 \\lor \\overline{1}) = (1 \\lor 1 \\lor 0) = 1$ ✓\n",
    "- Clause 2: $(1 \\lor \\overline{1} \\lor 1) = (1 \\lor 0 \\lor 1) = 1$ ✓  \n",
    "- Clause 3: $(\\overline{1} \\lor 1 \\lor 1) = (0 \\lor 1 \\lor 1) = 1$ ✓\n",
    "\n",
    "All clauses are satisfied, so our differentiable SAT solver successfully found a valid solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 [0.000300008513 0.9997 0.000300008513]\n",
      "1000 0.372614384 [0.202071369 0.809448719 0.202071369]\n",
      "2000 0.302438021 [0.317299336 0.773605466 0.317299306]\n",
      "3000 0.217134744 [0.467763871 0.900396705 0.467763722]\n",
      "4000 0.101519562 [0.686918437 1.02080131 0.686917782]\n",
      "5000 0.012053174 [0.915573061 1.00370216 0.915572584]\n",
      "6000 2.50427984e-05 [0.996464252 1.00000632 0.996464193]\n",
      "7000 2.62632721e-10 [0.999989033 1.00000441 0.999989033]\n",
      "8000 2.00159576e-11 [0.999997437 1.00000262 0.999997437]\n",
      "9000 7.20489597e-12 [0.99999845 1.00000155 0.99999845]\n"
     ]
    }
   ],
   "source": [
    "candidate = tf.Variable([0, 1, 0], dtype=tf.float32)\n",
    "problem_encoding = tf.Variable(\n",
    "    [\n",
    "        [0, 0, 1],\n",
    "        [0, 1, 0],\n",
    "        [1, 0, 0],\n",
    "    ],\n",
    "    dtype=tf.float32,\n",
    ")\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-4)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step():\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = circuit_forward_pass(candidate, problem_encoding)\n",
    "        loss = loss_fn(candidate, z)\n",
    "    grads = tape.gradient(loss, candidate)\n",
    "    opt.apply_gradients(zip([grads], [candidate]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "for i in range(10000):\n",
    "    loss = train_step()\n",
    "    if i % 1000 == 0:\n",
    "        tf.print(i, loss, candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "$X_1 = 1.0$, $X_2 = 1.0$ and $X_3 = 1.0$ is a valid candidate which we can verify manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.round(candidate))\n",
    "z = circuit_forward_pass(tf.round(candidate), problem_encoding)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f1d83f9e8f5c8837e86d6304fd35fdd4149e22cdc219dfc99f4444b7e631426b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

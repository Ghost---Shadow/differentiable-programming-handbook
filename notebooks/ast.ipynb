{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Differentiable Abstract Syntax Trees (ASTs)\n\nThis notebook demonstrates how to implement differentiable Abstract Syntax Trees using TensorFlow. We'll explore how to make tree operations differentiable by using soft selections and gradient-friendly stack operations.\n\n## Overview\n\nTraditional ASTs are discrete structures that don't allow gradient flow. By replacing discrete operations with continuous approximations, we can train neural networks to generate and manipulate syntactic structures end-to-end.\n\n## Initial Setup\n\nFirst, let's import the necessary libraries:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Phi Function - The \"Empty\" Detector\n\nThe `is_phi` function determines how \"close\" an element is to the empty symbol (φ). This is crucial for differentiable operations as it allows us to softly decide whether to perform operations based on how \"empty\" an element is.\n\n- When an element exactly matches the first basis vector [1,0,0,...], it returns 1 (indicating it's the empty symbol)\n- For other elements, it returns the dot product after L2 normalization\n- The gradient flows through the normalization, allowing the network to learn when elements should be treated as empty",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Stack Operations\n\nNow let's import our differentiable stack operations. These allow us to maintain stack-like data structures while preserving gradient flow:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Safe Push Operation\n\nThe `safe_push` function implements a differentiable stack push operation. Instead of discretely deciding whether to push or not, it uses the `is_phi` function to blend between the old and new stack states:\n\n- `t = is_phi_fn(element)`: Determines how \"empty\" the element is\n- If `t` is close to 1 (element is empty), the stack remains unchanged\n- If `t` is close to 0 (element is not empty), the element gets pushed\n- The blending `t * old + (1-t) * new` allows gradients to flow through both paths\n\nThis is a key insight: instead of discrete conditional operations, we use weighted combinations that preserve differentiability.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Pop and Purge Operation\n\nThe `pop_and_purge` function demonstrates a more complex stack operation that:\n1. Pops an element from the stack\n2. Pushes a phi (empty) element\n3. Immediately pops the phi element\n\nThis creates a \"clean\" pop operation that maintains stack structure while removing unwanted elements.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0 0 0]\n",
      "0 [1 0 0]\n",
      "0.707106769 [0.707106829 -0.707106709 0]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def is_phi(element):\n",
    "    tf.debugging.assert_rank(element, 1)\n",
    "    \n",
    "    elem_dim = tf.shape(element)[0]\n",
    "    phi = tf.one_hot(0, elem_dim)\n",
    "    \n",
    "    element = tf.math.l2_normalize(element)\n",
    "    t = tf.tensordot(element, phi, axes=1)\n",
    "\n",
    "    return t\n",
    "\n",
    "test1 = tf.Variable([1,0,0], dtype=tf.float32)\n",
    "test2 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "test3 = tf.Variable([.5,.5,0], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    result1 = is_phi(test1)\n",
    "    result2 = is_phi(test2)\n",
    "    result3 = is_phi(test3)\n",
    "\n",
    "tf.print(result1, tape.gradient(result1, test1))\n",
    "tf.print(result2, tape.gradient(result2, test2))\n",
    "tf.print(result3, tape.gradient(result3, test3))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Tensor Lookup Operations\n\nWe'll need 2D tensor lookup functionality for our grammar operations:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Grammar Definition\n\nHere we define a simple context-free grammar for generating expressions. The grammar has two parts:\n\n- **G_s**: Productions for the stack (what gets pushed back onto the stack)  \n- **G_o**: Productions for the output (what gets written to the output stream)\n\n### Token Definitions:\n- **PHI** (φ): Empty symbol  \n- **S**: Start symbol\n- **O**: Open expression\n- **T**: Terminal symbol  \n- **X**: Variable (x)\n- **PLUS**: Addition operator (+)\n\n### Grammar Rules:\nThe grammar matrices encode production rules where `G_s[production][token]` gives the tokens to push onto the stack, and `G_o[production][token]` gives the tokens to output.\n\nFor example, when we see 'S' and apply production 2, we push `[S, O, T]` back onto the stack.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Pretty Printing Utility\n\nA helper function to convert one-hot encoded tokens back to readable symbols for debugging:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Production Step\n\nThe `production_step` function implements one step of grammar-based generation:\n\n1. **Pop**: Remove the top token from the stack\n2. **Lookup**: Use the current production and popped token to look up what to do in the grammar\n3. **Push to Stack**: Push the tokens from `G_s` back onto the stack (in reverse order)  \n4. **Push to Output**: Push the tokens from `G_o` to the output stream\n\nThis simulates how a pushdown automaton processes a context-free grammar, but in a differentiable way.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.stacks import stack_push, stack_pop, stack_peek, new_stack, new_stack_from_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Debugging Utility\n\nA helper function to print detailed information about each production step, showing:\n- Which production rule was applied\n- What tokens were looked up in the grammar\n- Current state of stack and output",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Generate Function\n\nThe `generate` function orchestrates the complete generation process:\n\n1. **Initialize**: Create empty stack and output buffers\n2. **Start**: Push the start symbol 'S' onto the stack  \n3. **Process**: Apply each production rule in sequence\n4. **Return**: The generated output sequence and final stack state\n\nThis function effectively implements a differentiable parser/generator that can produce structured output following the grammar rules.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Example Generation\n\nLet's see the grammar in action! We'll apply a sequence of production rules `[2, 3, 0, 1, 0]` starting from the symbol 'S' and observe how it generates an expression step by step.\n\nThe debug output shows:\n- Which production rule is applied at each step\n- What gets pushed onto the stack (G_s lookups)  \n- What gets added to the output (G_o lookups)\n- Current stack and output states",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Token Encoding Utility\n\nA helper function to convert string expressions into one-hot encoded token sequences. This is useful for creating training targets and comparing generated outputs.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Soft Generation with Loss\n\nHere we demonstrate the fully differentiable version:\n\n1. **Soften Everything**: Apply softmax to grammar rules, production sequences, and symbols\n2. **Generate**: Run the soft generation process  \n3. **Compute Loss**: Compare generated output with expected target using cross-entropy loss\n4. **Gradients**: Show that gradients flow back through the entire generation process\n\nThis is the key insight: by making all discrete operations continuous, we can train end-to-end using standard gradient descent!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Training Step\n\nNow we implement a complete training step using Adam optimizer. The training process:\n\n1. **Forward Pass**: Generate output using current production sequence\n2. **Loss Calculation**: Compare with target output and desired final stack state  \n3. **Backward Pass**: Compute gradients with respect to production parameters\n4. **Update**: Apply gradients using Adam optimizer\n\nNotice we can train the production sequence itself - the network learns which grammar rules to apply!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Training Loop\n\nFinally, let's train the model to generate the target expression \"x + x\". We start with an initial production sequence and let the optimizer find the correct sequence of grammar rules.\n\nThe output shows the loss decreasing and the generated output converging to the target expression!",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ \n",
      "S _ _ \n",
      "S _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "S _ _ \n",
      "S _ _ \n",
      "S _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "S _ _ \n",
      "S O O \n",
      "S O O \n",
      "--------------------------------------------------------------------------------\n",
      "S O O \n",
      "S O S \n",
      "S O S \n",
      "--------------------------------------------------------------------------------\n",
      "[[0 1 0]\n",
      " [0.0428932235 0.0428932235 0.707106769]\n",
      " [0 0.707106769 0.0857864469]]\n",
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "[0.707106769 0.292893231 0]\n",
      "[1 0 0]\n",
      "[0.0606601834 0.792893231 0.792893231]\n",
      "([[0 0 0]\n",
      " [0.207106784 0.207106784 0.207106784]\n",
      " [0.207106799 0.207106799 0.207106799]], [2.87867975 1.53553391 1.76776707])\n"
     ]
    }
   ],
   "source": [
    "# @tf.function\n",
    "def safe_push(stack, element, is_phi_fn):\n",
    "    tf.debugging.assert_rank_at_least(stack[0], 2)\n",
    "    tf.debugging.assert_rank(stack[1], 1)\n",
    "    tf.debugging.assert_equal(tf.shape(stack[0])[1:], tf.shape(element))\n",
    "    tf.debugging.assert_equal(tf.rank(stack[0]) - 1, tf.rank(element) )\n",
    "    \n",
    "    t = is_phi_fn(element)\n",
    "    \n",
    "    old_buffer, old_index = stack\n",
    "    new_buffer, new_index = stack_push(stack, element)\n",
    "\n",
    "    buffer = t * old_buffer + (1 - t) * new_buffer\n",
    "    index = t * old_index + (1 - t) * new_index\n",
    "    \n",
    "    tf.print(tokens_pretty_print(old_buffer))\n",
    "    tf.print(tokens_pretty_print(new_buffer))\n",
    "    tf.print(tokens_pretty_print(buffer))\n",
    "    tf.print('-'*80)\n",
    "\n",
    "    # Hack to tell tensorflow that the shape has not changed\n",
    "    # TODO: Why does this hack work?\n",
    "    buffer = tf.reshape(buffer, tf.shape(old_buffer))\n",
    "    index = tf.reshape(index, tf.shape(old_index))\n",
    "\n",
    "    new_stack = (buffer, index)\n",
    "\n",
    "    return new_stack\n",
    "\n",
    "stack = new_stack((3,3), True)\n",
    "original_stack = stack\n",
    "\n",
    "element1 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "element2 = tf.Variable([0.5,0.5,0], dtype=tf.float32)\n",
    "element3 = tf.Variable([0,0,1], dtype=tf.float32)\n",
    "element4 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    stack = safe_push(stack, element1, is_phi)\n",
    "    stack = safe_push(stack, element2, is_phi)\n",
    "    stack = safe_push(stack, element3, is_phi)\n",
    "    stack = safe_push(stack, element4, is_phi)\n",
    "    \n",
    "tf.print(stack[0])\n",
    "tf.print(tf.round(stack[0]))\n",
    "tf.print(stack[1])\n",
    "tf.print(tf.round(stack[1]))\n",
    "tf.print(tape.gradient(stack[0], element3))\n",
    "tf.print(tape.gradient(stack, original_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([[1 1 1]\n",
      " [1 1 1]\n",
      " [1 0 0]], [0 0 1])\n",
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def pop_and_purge(stack, phi):\n",
    "    stack_len = tf.shape(stack[0])[1]\n",
    "    stack, element = stack_pop(stack)\n",
    "    stack = stack_push(stack, phi)\n",
    "    stack, _ = stack_pop(stack)\n",
    "    \n",
    "    return stack, element\n",
    "\n",
    "stack = new_stack_from_buffer(tf.ones((3,3), dtype=tf.float32))\n",
    "phi = tf.one_hot(0, 3, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    stack, element = pop_and_purge(stack, phi)\n",
    "    \n",
    "tf.print(stack)\n",
    "tf.print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 0 1]\n",
      " [0 1 0]]\n",
      "[1 0 0]\n",
      "[-1 1 1]\n",
      "([[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]], [4 1 1])\n"
     ]
    }
   ],
   "source": [
    "stack = new_stack((3,3), True)\n",
    "\n",
    "element1 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "element2 = tf.Variable([1,0,0], dtype=tf.float32)\n",
    "element3 = tf.Variable([0,0,1], dtype=tf.float32)\n",
    "element4 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "\n",
    "original_stack = stack\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    stack = safe_push(stack, element1, is_phi)\n",
    "    stack = safe_push(stack, element2, is_phi)\n",
    "    stack = safe_push(stack, element3, is_phi)\n",
    "    stack = safe_push(stack, element4, is_phi)\n",
    "    \n",
    "tf.print(stack[0])\n",
    "tf.print(stack[1])\n",
    "tf.print(tape.gradient(stack[0], element3))\n",
    "tf.print(tape.gradient(stack, original_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.array_ops import tensor_lookup_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_DIM = 6\n",
    "PRODUCTION_DIM = 4\n",
    "STACK_SIZE = 10\n",
    "PHI = np.eye(TOKEN_DIM)[0]\n",
    "S = np.eye(TOKEN_DIM)[1]\n",
    "O = np.eye(TOKEN_DIM)[2]\n",
    "T = np.eye(TOKEN_DIM)[3]\n",
    "X = np.eye(TOKEN_DIM)[4]\n",
    "PLUS = np.eye(TOKEN_DIM)[5]\n",
    "\n",
    "E = [PHI, PHI, PHI]\n",
    "\n",
    "G_s = tf.constant([\n",
    "    [E, E, E, E, E, E],\n",
    "    [E, E, E, E, E, E],\n",
    "    [E, [S, O, T], E, E, E, E],\n",
    "    [E, [T, PHI, PHI], E, E, E, E],\n",
    "], dtype=tf.float32)\n",
    "G_o = tf.constant([\n",
    "    [E, E, E, [X, PHI, PHI], E, E],\n",
    "    [E, E, [PLUS, PHI, PHI], E, E, E],\n",
    "    [E, E, E, E, E, E],\n",
    "    [E, E, E, E, E, E],\n",
    "], dtype=tf.float32)\n",
    "grammar = (G_s, G_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'_ S O T x + '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokens_pretty_print(tokens):\n",
    "    tokens = tf.argmax(tokens, axis=1)\n",
    "    lookup = ['_', 'S', 'O', 'T', 'x', '+']\n",
    "    \n",
    "    result = ''\n",
    "    \n",
    "    for token in tokens:\n",
    "        result += f'{lookup[token]} '\n",
    "        \n",
    "    return result\n",
    "\n",
    "tokens = tf.transpose(tf.one_hot([0,1,2,3,4,5], TOKEN_DIM, dtype=tf.float32))\n",
    "tokens_pretty_print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T O S _ _ _ _ _ _ _ \n",
      "[-0.0833333358 -0.0833333358 3 -0.0416666679]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def production_step(grammar, production, stack, output, phi, is_phi_fn):\n",
    "    tf.debugging.assert_rank(grammar[0], 4)\n",
    "    tf.debugging.assert_rank(grammar[1], 4)\n",
    "    tf.debugging.assert_rank(production, 1)\n",
    "    tf.debugging.assert_rank(stack[0], 2)\n",
    "    tf.debugging.assert_rank(output[0], 2)\n",
    "    \n",
    "    G_s, G_o = grammar\n",
    "    \n",
    "    # Save the shapes\n",
    "    stack_0_shape = tf.shape(stack[0])\n",
    "    stack_1_shape = tf.shape(stack[1])\n",
    "    output_0_shape = tf.shape(output[0])\n",
    "    output_1_shape = tf.shape(output[1])\n",
    "    \n",
    "    # Get next token from stack\n",
    "    stack, stack_top_token = pop_and_purge(stack, phi)\n",
    "\n",
    "    # Push tokens back onto the stack\n",
    "    tokens_to_push = tensor_lookup_2d(G_s, production, stack_top_token)\n",
    "    for token in tf.reverse(tokens_to_push, axis=[0]):\n",
    "        stack = safe_push(stack, token, is_phi_fn)\n",
    "    \n",
    "    # Push tokens to output\n",
    "    tokens_to_push = tensor_lookup_2d(G_o, production, stack_top_token)\n",
    "    for token in tokens_to_push:\n",
    "        output = safe_push(output, token, is_phi_fn)\n",
    "    \n",
    "    return stack, output\n",
    "\n",
    "stack = new_stack(((STACK_SIZE, TOKEN_DIM)))\n",
    "output = new_stack(((STACK_SIZE, TOKEN_DIM)))\n",
    "\n",
    "stack = safe_push(stack, tf.constant(S, dtype=tf.float32), is_phi)\n",
    "production = tf.one_hot(2, PRODUCTION_DIM)\n",
    "phi = tf.one_hot(0, TOKEN_DIM, dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    tape.watch(grammar)\n",
    "    tape.watch(production)\n",
    "    tape.watch(stack)\n",
    "    tape.watch(output)\n",
    "    \n",
    "    new_s, new_o = production_step(grammar, production, stack, output, phi, is_phi)\n",
    "\n",
    "tf.print(tokens_pretty_print(new_s[0]))\n",
    "# tf.print(tape.gradient(new_o, output))\n",
    "# tf.print(tape.gradient(new_s, stack))\n",
    "# tf.print(tape.gradient(new_s[0], grammar[0]).shape)\n",
    "# tf.print(tape.gradient(new_s[1], grammar[0]).shape)\n",
    "# tf.print(tape.gradient(new_o[0], grammar[1]).shape)\n",
    "# tf.print(tape.gradient(new_o[1], grammar[1]).shape)\n",
    "tf.print(tape.gradient(new_s, production))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _ _ _ _ _ _ \n",
      "S _ _ _ _ _ _ _ _ _ \n",
      "S _ _ _ _ _ _ _ _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "--------------------------------------------------------------------------------\n",
      "S _ _ _ _ _ _ _ _ _ \n",
      "_ _ _ _ _ _ _ _ _ _ \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "stack_shape = (STACK_SIZE, TOKEN_DIM)\n",
    "\n",
    "phi = tf.one_hot([0], TOKEN_DIM, dtype=tf.float32)\n",
    "soft_phi = tf.nn.softmax(phi, axis=-1)\n",
    "stack_buffer = tf.tile(soft_phi, (stack_shape[0], 1))\n",
    "soft_stack = new_stack_from_buffer(stack_buffer)\n",
    "output_buffer = tf.tile(soft_phi, (stack_shape[0], 1))\n",
    "soft_output = new_stack_from_buffer(output_buffer)\n",
    "soft_s = tf.nn.softmax(tf.constant(S, dtype=tf.float32))\n",
    "\n",
    "soft_stack = safe_push(soft_stack, soft_s, is_phi)\n",
    "soft_p = tf.nn.softmax(tf.one_hot(2, PRODUCTION_DIM))\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    tape.watch(grammar)\n",
    "    tape.watch(production)\n",
    "    tape.watch(stack)\n",
    "    tape.watch(output)\n",
    "    \n",
    "    # Soften the grammar\n",
    "    gs, go = grammar\n",
    "    sgs, sgo = tf.nn.softmax(gs), tf.nn.softmax(go)\n",
    "    soft_g = (sgs, sgo)\n",
    "\n",
    "    new_s, new_o = production_step(soft_g, soft_p, soft_stack, soft_output, soft_phi[0], is_phi)\n",
    "# tf.config.experimental_run_functions_eagerly(False)\n",
    "\n",
    "tf.print(tokens_pretty_print(soft_stack[0]))\n",
    "#here\n",
    "tf.print(tokens_pretty_print(new_s[0]))\n",
    "# tf.print(tape.gradient(new_o, output))\n",
    "# tf.print(tape.gradient(new_s, stack))\n",
    "# tf.print(tape.gradient(new_s[0], grammar[0]).shape)\n",
    "# tf.print(tape.gradient(new_s[1], grammar[0]).shape)\n",
    "# tf.print(tape.gradient(new_o[0], grammar[1]).shape)\n",
    "# tf.print(tape.gradient(new_o[1], grammar[1]).shape)\n",
    "tf.print(tape.gradient(new_s, production))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_step_info(grammar, production, stack, output):\n",
    "    gs, go = grammar\n",
    "    top = stack_peek(stack)\n",
    "    tf.print('p\\t', tf.argmax(production))\n",
    "    i = tf.argmax(production)\n",
    "    j = tf.argmax(top)\n",
    "    tf.print('G_s\\t', tokens_pretty_print(gs[i][j]), (i,j))\n",
    "    tf.print('G_o\\t', tokens_pretty_print(go[i][j]), (i,j))\n",
    "    tf.print('S_i+1\\t', tokens_pretty_print(stack[0]), tf.argmax(stack[1]))\n",
    "    tf.print('O_i+1\\t', tokens_pretty_print(output[0]), tf.argmax(output[1]))\n",
    "    tf.print('-'*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def generate(grammar, productions, stack_shape, S, phi, is_phi_fn, print_steps=False):\n",
    "    # Reserve space for stack and output\n",
    "    stack_buffer = tf.tile(phi, (stack_shape[0], 1))\n",
    "    stack = new_stack_from_buffer(stack_buffer)\n",
    "    output_buffer = tf.tile(phi, (stack_shape[0], 1))\n",
    "    output = new_stack_from_buffer(output_buffer)\n",
    "    \n",
    "    # Push S to top of stack\n",
    "    stack = safe_push(stack, S, is_phi)\n",
    "    \n",
    "    productions = tf.unstack(productions)\n",
    "\n",
    "    for production in productions:\n",
    "        stack, output = production_step(grammar, production, stack, output, phi[0], is_phi_fn)\n",
    "        \n",
    "        if print_steps:\n",
    "            dump_step_info(grammar, production, stack, output)\n",
    "        \n",
    "    return tf.reverse(output[0], axis=[0]), stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "productions = tf.one_hot([2, 3, 0, 1, 0], PRODUCTION_DIM)\n",
    "\n",
    "stack_shape = (STACK_SIZE, TOKEN_DIM)\n",
    "d_S = tf.constant(S, dtype=tf.float32)\n",
    "d_phi = tf.constant(tf.one_hot([0], TOKEN_DIM))\n",
    "\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    tape.watch(productions)\n",
    "    output, final_stack = generate(grammar, productions, stack_shape, d_S, d_phi, is_phi, True)\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(False)\n",
    "tf.print('Final result:')\n",
    "tf.print(tokens_pretty_print(output))\n",
    "tf.print('-'*80)\n",
    "tf.print('Final stack:')\n",
    "tf.print(tokens_pretty_print(final_stack))\n",
    "tf.print('-'*80)\n",
    "tf.print(tape.gradient(output, productions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_to_tokens(s, token_dim, total_length):\n",
    "    lookup = ['_', 'S', 'O', 'T', 'x', '+']\n",
    "    arr = []\n",
    "    for t in s.split(' '):\n",
    "        arr.append(lookup.index(t))\n",
    "    \n",
    "    phi = lookup.index('_')\n",
    "    arr = ([phi] * (total_length - len(arr))) + arr\n",
    "        \n",
    "    return tf.one_hot(arr, token_dim)\n",
    "\n",
    "encode_to_tokens('x + x +', TOKEN_DIM, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "productions = tf.one_hot([2, 3, 0, 1, 0], PRODUCTION_DIM)\n",
    "\n",
    "stack_shape = (STACK_SIZE, TOKEN_DIM)\n",
    "d_S = tf.constant(S, dtype=tf.float32)\n",
    "d_phi = tf.constant(tf.one_hot([0], TOKEN_DIM))\n",
    "expected_output = encode_to_tokens('x + x +', TOKEN_DIM, STACK_SIZE)\n",
    "zero_stack = tf.one_hot([0] * stack_shape[0], stack_shape[1], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent = True) as tape:\n",
    "    tape.watch(productions)\n",
    "    # Soften the grammar\n",
    "    gs, go = grammar\n",
    "    sgs, sgo = tf.nn.softmax(gs), tf.nn.softmax(go)\n",
    "    soft_g = (sgs, sgo)\n",
    "\n",
    "    # Soften the productions\n",
    "    soft_p = tf.nn.softmax(productions,axis=-1)\n",
    "\n",
    "    # Soften S\n",
    "    soft_s = tf.nn.softmax(d_S)\n",
    "\n",
    "    soft_phi = tf.nn.softmax(d_phi, axis=-1)\n",
    "    \n",
    "    output_, stack_ = generate(soft_g, soft_p, stack_shape, soft_s, soft_phi, is_phi, True)\n",
    "\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(output, output_))\n",
    "    loss += tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(zero_stack, stack_))\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(False)\n",
    "tf.print('Final result:')\n",
    "tf.print(tokens_pretty_print(output_))\n",
    "tf.print('-'*80)\n",
    "tf.print('Final stack:')\n",
    "tf.print(tokens_pretty_print(stack_))\n",
    "tf.print('-'*80)\n",
    "tf.print(loss)\n",
    "tf.print(tape.gradient(loss, productions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(1e-1)\n",
    "\n",
    "@tf.function\n",
    "def train_step(grammar, productions, stack_shape, S, is_phi_fn, output, print_steps=False):\n",
    "    zero_stack = tf.one_hot([0] * stack_shape[0], stack_shape[1], dtype=tf.float32)\n",
    "    phi = tf.one_hot([0], stack_shape[1], dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(productions)\n",
    "#         # Soften the grammar\n",
    "#         gs, go = grammar\n",
    "#         sgs, sgo = tf.nn.softmax(gs), tf.nn.softmax(go)\n",
    "#         soft_g = (sgs, sgo)\n",
    "        \n",
    "#         # Soften the productions\n",
    "#         soft_p = tf.nn.softmax(productions,axis=-1)\n",
    "        \n",
    "#         # Soften S\n",
    "#         soft_s = tf.nn.softmax(S)\n",
    "        \n",
    "#         soft_phi = tf.nn.softmax(phi, axis=-1)\n",
    "        \n",
    "#         output_, stack_ = generate(soft_g, soft_p, stack_shape, soft_s, soft_phi, is_phi_fn, True)\n",
    "        \n",
    "#         loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(output, output_))\n",
    "#         loss += tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(zero_stack, stack_))\n",
    "        \n",
    "        output_, stack_ = generate(grammar, productions, stack_shape, S, phi, is_phi_fn, print_steps)\n",
    "        loss = tf.nn.l2_loss(output - output_) + tf.nn.l2_loss(zero_stack - stack_)\n",
    "        \n",
    "    grads = tape.gradient(loss, productions)\n",
    "    opt.apply_gradients(zip([grads], [productions]))\n",
    "    \n",
    "    return loss, output_, stack_\n",
    "\n",
    "MAX_PRODUCTIONS = 5\n",
    "# productions = tf.Variable(tf.one_hot([0] * MAX_PRODUCTIONS, PRODUCTION_DIM), dtype=tf.float32)\n",
    "productions = tf.Variable(tf.one_hot([2, 3, 0, 1, 0], PRODUCTION_DIM, dtype=tf.float32))\n",
    "# productions = tf.Variable(tf.one_hot([2, 0, 0, 0, 0], PRODUCTION_DIM), dtype=tf.float32)\n",
    "stack_shape = (STACK_SIZE, TOKEN_DIM)\n",
    "d_S = tf.constant(S, dtype=tf.float32)\n",
    "output = encode_to_tokens('x + x', TOKEN_DIM, STACK_SIZE)\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "loss, output_, stack_ = train_step(grammar, productions, stack_shape, d_S, is_phi, output, True)\n",
    "tf.config.experimental_run_functions_eagerly(False)\n",
    "tf.print(loss)\n",
    "tf.print(tokens_pretty_print(output_), tokens_pretty_print(output))\n",
    "tf.print(tokens_pretty_print(stack_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# productions = tf.Variable(tf.one_hot([2, 3, 0, 1, 0], PRODUCTION_DIM), dtype=tf.float32)\n",
    "productions = tf.Variable(tf.one_hot([2, 3, 0, 0, 0], PRODUCTION_DIM), dtype=tf.float32)\n",
    "output = encode_to_tokens('x + x', TOKEN_DIM, STACK_SIZE)\n",
    "\n",
    "for var in opt.variables():\n",
    "    var.assign(tf.zeros_like(var))\n",
    "\n",
    "for i in range(100):\n",
    "    loss, output_, stack_ = train_step(grammar, productions, stack_shape, d_S, is_phi, output)\n",
    "    if i % 10 == 0:\n",
    "        p_output = tokens_pretty_print(output_)\n",
    "        p_stack = tokens_pretty_print(stack_)\n",
    "        \n",
    "        tf.print(loss, p_output, p_stack, tf.argmax(productions, axis=-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
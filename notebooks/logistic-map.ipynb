{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Differentiable Logistic Map\n\nThe logistic map is a classic example of how simple nonlinear dynamical systems can exhibit complex, chaotic behavior. This notebook explores how to make the logistic map differentiable, enabling gradient-based analysis and optimization of chaotic systems.\n\n## The Logistic Map\n\nThe logistic map is a discrete-time dynamical system defined by:\n\n$$x_{n+1} = r \\cdot x_n \\cdot (1 - x_n)$$\n\nwhere:\n- $x_n$ is the population at time step $n$ (normalized between 0 and 1)\n- $r$ is the growth rate parameter\n- The term $(1 - x_n)$ represents environmental resistance\n\n## Why Study This?\n\nThe logistic map demonstrates:\n- **Chaos theory**: How deterministic systems can produce unpredictable behavior\n- **Bifurcations**: How small parameter changes can drastically alter system behavior\n- **Sensitivity**: How initial conditions affect long-term outcomes\n\n## Making It Differentiable\n\nBy implementing the logistic map in TensorFlow, we can:\n- **Analyze sensitivity**: Compute gradients with respect to parameters and initial conditions\n- **Optimize parameters**: Find values that produce desired behaviors\n- **Study dynamics**: Understand how changes propagate through iterations\n- **Learn from chaos**: Train models that incorporate chaotic dynamics\n\nLet's explore these capabilities through differentiable implementations.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Gradient Analysis\n\nFor the logistic map $f(x) = r \\cdot x \\cdot (1-x)$, the analytical derivatives are:\n\n- $\\frac{\\partial f}{\\partial x} = r(1 - 2x)$\n- $\\frac{\\partial f}{\\partial r} = x(1-x)$\n\nLet's verify our TensorFlow implementation matches these analytical results:\n\n**Results**:\n- **Output**: $y = 3.0 \\times 0.3 \\times (1-0.3) = 0.63$ ✓\n- **Gradient w.r.t. x**: $3.0 \\times (1 - 2 \\times 0.3) = 1.2$ ✓  \n- **Gradient w.r.t. r**: $0.3 \\times (1-0.3) = 0.21$ ✓\n\nPerfect match! Our differentiable implementation correctly captures the dynamics.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Multi-step Evolution\n\nThe `chained_logistic_map` iterates the map multiple times, allowing us to study:\n- **Trajectory evolution**: How the system evolves over time\n- **Sensitivity analysis**: How perturbations in parameters/initial conditions affect final outcomes\n- **Lyapunov exponents**: Through gradient magnitudes (related to chaos)\n\n### Results Analysis\n\nAfter 5 iterations with $r=3.0$ and $x_0=0.3$:\n- **Final value**: $x_5 = 0.632$ (converging toward a fixed point)\n- **Sensitivity to initial condition**: $\\frac{\\partial x_5}{\\partial x_0} = 1.047$ \n- **Sensitivity to parameter**: $\\frac{\\partial x_5}{\\partial r} = 0.052$\n\n### Key Insights\n\n1. **Gradient accumulation**: The gradients represent the cumulative sensitivity across all iterations\n2. **Stability analysis**: Gradients close to 1 suggest stable dynamics; much larger values indicate chaos\n3. **Parameter sensitivity**: Small gradient w.r.t. $r$ suggests the system is relatively robust to parameter changes at this value\n\nThis differentiable approach enables quantitative analysis of dynamical systems that would be difficult with traditional methods!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Applications and Extensions\n\nThis differentiable logistic map implementation opens up numerous possibilities:\n\n### Chaos Analysis\n- **Bifurcation detection**: Train models to identify parameter values where behavior changes\n- **Lyapunov exponent estimation**: Use gradient magnitudes to quantify chaos\n- **Basin boundary analysis**: Study how initial conditions affect long-term outcomes\n\n### Optimization Applications\n- **Parameter fitting**: Given observed time series, find the $r$ value that best fits\n- **Control problems**: Learn control inputs to drive chaotic systems to desired states\n- **Ensemble forecasting**: Optimize initial condition distributions for prediction\n\n### Machine Learning Integration\n- **Reservoir computing**: Use chaotic dynamics as computational substrates\n- **Physics-informed networks**: Incorporate chaotic constraints into neural architectures  \n- **Generative models**: Learn to produce realistic chaotic time series\n\n### Research Directions\n- **Higher-dimensional maps**: Extend to systems like the Hénon map\n- **Stochastic versions**: Add differentiable noise to study noisy chaos\n- **Coupled systems**: Networks of interacting logistic maps\n- **Control theory**: Differentiable chaos control and synchronization\n\n## Summary\n\nWe've successfully implemented a differentiable version of the logistic map that:\n\n- ✅ **Preserves dynamics**: Maintains the mathematical properties of the original system\n- ✅ **Enables analysis**: Provides gradient-based sensitivity analysis  \n- ✅ **Supports optimization**: Allows parameter learning and control\n- ✅ **Facilitates research**: Opens new avenues for studying chaotic systems\n\nThis represents a powerful fusion of dynamical systems theory with modern differentiable programming, enabling new approaches to understanding and controlling complex nonlinear systems.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Chained Iterations: Studying Long-term Dynamics\n\nNow let's examine what happens when we iterate the logistic map multiple times. This is where the interesting dynamics emerge!",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Single Iteration Analysis\n\nLet's start with a single iteration of the logistic map and examine its gradients:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63\n",
      "1.19999981\n",
      "0.210000008\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def logistic_map(r, x_n):\n",
    "    return r * x_n * (1 - x_n)\n",
    "\n",
    "r = tf.Variable(3.0)\n",
    "x_n = tf.Variable(0.3)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = logistic_map(r, x_n)\n",
    "    \n",
    "tf.print(y)\n",
    "tf.print(tape.gradient(y, x_n))\n",
    "tf.print(tape.gradient(y, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.631621838\n",
      "1.04724312\n",
      "0.0521896482\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def chained_logistic_map(r, x_n, steps):\n",
    "    for _ in tf.range(steps):\n",
    "        x_n = logistic_map(r, x_n)\n",
    "        \n",
    "    return x_n\n",
    "\n",
    "r = tf.Variable(3.0)\n",
    "x_n = tf.Variable(0.3)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = chained_logistic_map(r, x_n, 5)\n",
    "    \n",
    "tf.print(y)\n",
    "tf.print(tape.gradient(y, x_n))\n",
    "tf.print(tape.gradient(y, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Differentiable Subset Sum Problem\n\nThe **subset sum problem** is a fundamental NP-complete problem in computational complexity theory: given a set of integers and a target sum, determine if there exists a subset whose elements sum exactly to the target value. While this problem is computationally intractable for large instances using classical algorithms, we can approach it using differentiable programming techniques.\n\n## Problem Definition\n\nThe [subset sum problem](https://en.wikipedia.org/wiki/Subset_sum_problem) is formally defined as follows: given a set of numbers $S = \\{s_1, s_2, ..., s_n\\}$ and a target value $T$, find a subset $S' \\subseteq S$ such that $\\sum_{s_i \\in S'} s_i = T$.\n\nIn our differentiable approach, we reformulate this as finding a binary selection mask $M = [m_1, m_2, ..., m_n]$ where $m_i \\in \\{0, 1\\}$ indicates whether element $s_i$ is included in the subset."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Mathematical Formulation\n\nLet's illustrate this with a concrete example. Given a set $S$ and a target $T$:\n\n$$T = 7.0$$\n\n$$S = \\begin{bmatrix} 1.0 & 2.0 & 3.0 & 4.0 & 5.0 \\end{bmatrix}$$\n\nOur goal is to find a binary mask $M$ such that the dot product equals the target:\n\n$$M \\cdot S = T$$\n\nHere's an example of a valid mask that achieves our target:\n\n$$M = \\begin{bmatrix} 0.0 & 0.0 & 1.0 & 1.0 & 0.0 \\end{bmatrix}$$\n\nWe can verify: $0 \\cdot 1 + 0 \\cdot 2 + 1 \\cdot 3 + 1 \\cdot 4 + 0 \\cdot 5 = 7$\n\n## Differentiable Implementation\n\nThe key insight is to make the subset sum computation fully differentiable using TensorFlow operations. This enables gradient-based optimization to search for valid solutions, even though the underlying problem is discrete."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import Model, Input, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7.0, shape=(), dtype=float32)\n",
      "tf.Tensor([0. 0. 1. 1. 0.], shape=(5,), dtype=float32)\n",
      "tf.Tensor([1. 2. 3. 4. 5.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def compute_subset_sum(S, M):\n",
    "    return tf.tensordot(S, M, 1)\n",
    "\n",
    "S = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "M = tf.Variable([0,0,1,1,0],dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    T_ = compute_subset_sum(S, M)\n",
    "    \n",
    "print(T_)\n",
    "print(tape.gradient(T_, S))\n",
    "print(tape.gradient(T_, M))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Naive Training Approach\n\nIf we train directly using only the L2 loss between predicted and target sums, we encounter a fundamental problem: **the optimizer finds continuous solutions rather than binary ones**. \n\nThe mask $M$ converges to fractional values (around 0.467 for each element in this case) rather than the desired binary values $\\{0, 1\\}$. This happens because the loss function only cares about achieving the target sum, not about maintaining the discrete nature of the selection.\n\n### The Problem with Continuous Relaxation\n\nWhen we relax the discrete constraint $m_i \\in \\{0, 1\\}$ to $m_i \\in [0, 1]$, the optimizer finds the \"easy\" solution of using fractional coefficients. While this achieves the target sum mathematically, it doesn't represent a valid subset selection.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "However, if we train as is, we find that $M$ is not a mask but it forms a linear combination with its inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Enforcing Binary Solutions with Bistable Loss\n\nTo force the mask values toward binary solutions, we introduce the **bistable loss function**. This loss function penalizes values that are neither close to 0 nor close to 1, creating an energy landscape that favors binary solutions.\n\nThe bistable loss is defined as:\n$$L_{bistable}(x) = 4x(1-x)$$\n\nThis function:\n- **Minimizes** when $x = 0$ or $x = 1$ (binary values)\n- **Maximizes** when $x = 0.5$ (maximally uncertain values)\n- Creates a **double-well potential** that encourages binary convergence\n\nBy adding this regularization term to our primary loss, we guide the optimization toward discrete solutions while still allowing gradient-based training."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bistable loss\n",
    "To force the values to be close to 0 and 1, we introduce the [Bistable Loss](notebooks/boolean-satisfiability.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Results with Bistable Loss\n\nThe bistable loss significantly improves convergence toward binary solutions. Observing the training output, we can see that:\n\n1. **Initial iterations**: Values start near 1.0 and gradually differentiate\n2. **Middle iterations**: Some values move toward 0, others toward 1\n3. **Final iterations**: Values converge much closer to binary solutions\n\nThe combination of subset sum loss and bistable loss creates a more constrained optimization landscape that favors valid discrete solutions while maintaining differentiability for gradient-based training."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On retraining we find that each element the mask is now closer to 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## One-Hot Softmax Approach\n\nTo further enforce binary constraints, we employ a **one-hot encoding with softmax** strategy. This approach transforms the problem into a more structured discrete choice.\n\n### Dimensional Transformation\n\nWe expand the mask dimensionality from:\n$$M = \\begin{bmatrix} 0 & 0 & 1 & 1 & 0 \\end{bmatrix}$$\n\nto a 2D representation:\n$$M = \\begin{bmatrix} 1 & 1 & 0 & 0 & 1 \\\\ 0 & 0 & 1 & 1 & 0 \\end{bmatrix}$$\n\nwhere each column represents a binary choice between \"exclude\" (first row) and \"include\" (second row).\n\n### Softmax Normalization\n\nBy applying softmax along the vertical axis, we ensure that each element has exactly one active choice:\n$$M_s = \\text{softmax}(M, \\text{axis}=\\text{vertical})$$\n\n### Modified Computation\n\nThe subset sum computation becomes:\n$$\\bar{T} = \\frac{(M_s[1] \\cdot S) + ((1 - M_s[0]) \\cdot S)}{2}$$\n\nThis formulation enforces mutual exclusivity between inclusion and exclusion decisions while maintaining differentiability."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot softmax\n",
    "\n",
    "To further make sure that the mask remains either 0 or 1, we increase the dimentionality of the $M$ and apply softmax along the vertical axis.\n",
    "\n",
    "\\begin{equation*}\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 1 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "becomes\n",
    "\n",
    "\\begin{equation*}\n",
    "M = \n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 0 & 0 & 1 \\\\\n",
    "0 & 0 & 1 & 1 & 0 \\\\\n",
    "\\end{bmatrix}\\end{equation*}\n",
    "\n",
    "Therefore, $\\bar{T}$ becomes\n",
    "\n",
    "\\begin{equation*} \n",
    "M_s = softmax(M, axis=vertical)\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "\\bar{T} = \\frac{(M_s[1] \\cdot S) + ((1 - M_s[0]) \\cdot S)}{2} \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 1]\n",
      " [0 0 1 1 0]]\n",
      "7.2689414\n",
      "[0.268941402 0.268941402 0.731058598 0.731058598 0.268941402]\n",
      "[[-0.196611926 0.196611941]\n",
      " [-0.393223852 0.393223882]\n",
      " [-0.589835823 0.589835823]\n",
      " [-0.786447763 0.786447704]\n",
      " [-0.983059645 0.983059704]]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def compute_subset_sum_v2(S, M):\n",
    "    M = tf.transpose(M)\n",
    "    pos = tf.tensordot(S, M[1], 1)\n",
    "    neg = tf.tensordot(S, 1 - M[0], 1)\n",
    "    return (pos + neg) / 2\n",
    "\n",
    "S = tf.Variable([1,2,3,4,5],dtype=tf.float32)\n",
    "M = tf.Variable(tf.one_hot([0,0,1,1,0], 2),dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    M_s = tf.nn.softmax(M, axis=1)\n",
    "    T_ = compute_subset_sum_v2(S, M_s)\n",
    "\n",
    "tf.print(tf.transpose(M))\n",
    "tf.print(T_)\n",
    "tf.print(tape.gradient(T_, S))\n",
    "tf.print(tape.gradient(T_, M))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Comparative Performance\n\nThe combination of **bistable loss with one-hot softmax encoding** demonstrates superior performance compared to the naive approach:\n\n1. **Faster Convergence**: The structured choice representation accelerates training\n2. **Better Binary Solutions**: Values converge more reliably to near-binary states\n3. **Stability**: The softmax constraint prevents gradient explosion and provides more stable training dynamics\n\nThe one-hot approach transforms the continuous relaxation problem into a structured discrete choice problem while maintaining end-to-end differentiability."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Neural Network-Driven Candidate Generation\n\nIn the previous experiments, we manually initialized the mask $M$. Now we delegate this **candidate generation** to a neural network, creating an end-to-end learnable system.\n\n### Architecture Overview\n\nLet $G$ be our **candidate generator network**. The network takes a set $S$ and target $T$ as input and produces an initial guess $\\bar{M}$:\n\n$$\\bar{M} = G(S, T)$$\n\n### End-to-End Differentiable Pipeline\n\nThe complete system becomes:\n$$\\bar{T} = H(S, G(S, T))$$\n\nwhere:\n- $G$: Neural network that generates subset selection candidates\n- $H$: Differentiable subset sum computation function\n- $S$: Input set\n- $T$: Target sum\n- $\\bar{T}$: Predicted sum\n\n### Training Strategy\n\nThe neural network learns to generate good initial candidates by optimizing:\n1. **Subset Sum Loss**: $|\\bar{T} - T|$ to match the target\n2. **Bistable Loss**: To encourage binary selections in the generated mask\n\nThis approach combines the power of **neural pattern recognition** (for generating candidates) with **differentiable algorithms** (for computing subset sums), creating a hybrid system that can tackle NP-complete problems through gradient-based optimization."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network driven candidate generation\n",
    "\n",
    "In the last few experiments, the initial candidate $M$ was arbitarily chosen. Here, we shall now delegate this *guesswork* to a neural network.\n",
    "\n",
    "Let $G$ be our DNN. The specifics of its architecture is not relevant as this is a toy problem. $G$ takes in an set $S$ and a target $T$ and tries to guess the correct answer $\\bar{M}$ in one shot.\n",
    "\n",
    "$$ \\bar{M} = G(S, T) $$\n",
    "\n",
    "A benefit of having the `compute_subset_sum` function (lets call it $H$) end-to-end differentiable is that it can now be interfaced seamlessly with DNNs. \n",
    "\n",
    "We get\n",
    "\n",
    "\\begin{equation*} \n",
    "\\bar{T} = H(S, \\bar{M})\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*} \n",
    "or\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "\\bar{T} = H(S, G(S, T)) \n",
    "\\end{equation*}\n",
    "\n",
    "Now, we can use our bistable loss and $| \\bar{T} - T | $ loss to train the system end-to-end."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Candidate Generator Architecture\n\nThe `CandidateGeneratorBlock` is a simple feedforward network that:\n\n1. **Input Processing**: Takes concatenated set and target `[S, T]` as input\n2. **Feature Learning**: Uses two hidden layers with ReLU activation for feature extraction\n3. **Output Generation**: Produces logits for binary choices and applies softmax\n4. **Shape Transformation**: Reshapes output to `(batch_size, set_length, 2)` for one-hot encoding\n\nThe network learns to map from `(set + target)` pairs to binary selection patterns, effectively learning heuristics for the subset sum problem.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateGeneratorBlock(layers.Layer):\n",
    "    def __init__(self, set_length):\n",
    "        super(CandidateGeneratorBlock, self).__init__()\n",
    "        self.set_length = set_length\n",
    "        \n",
    "        self.dense1 = layers.Dense(10, kernel_initializer=\"he_normal\", activation='relu')\n",
    "        self.dense2 = layers.Dense(10, kernel_initializer=\"he_normal\", activation='relu')\n",
    "        self.dense3 = layers.Dense(set_length * 2)\n",
    "\n",
    "    def call(self, x):\n",
    "        h = x\n",
    "        h = self.dense1(h)\n",
    "        h = self.dense2(h)\n",
    "        h = self.dense3(h)\n",
    "        h = tf.reshape(h, (-1, self.set_length, 2))\n",
    "        h = tf.nn.softmax(h, axis=-1)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6) (1, 5, 2) (1, 5, 2)\n",
      "tf.Tensor(\n",
      "[[[0.7139912  0.28600878]\n",
      "  [0.47499245 0.5250076 ]\n",
      "  [0.4091633  0.59083676]\n",
      "  [0.27171898 0.7282811 ]\n",
      "  [0.57239777 0.42760226]]], shape=(1, 5, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "set_length = 5\n",
    "temp_generator = CandidateGeneratorBlock(set_length)\n",
    "input_shape = (batch_size, set_length + 1)\n",
    "output_shape = (batch_size, set_length, 2)\n",
    "x = tf.random.normal(input_shape)\n",
    "y = tf.math.round(tf.random.uniform(output_shape, minval=0, maxval=1))\n",
    "result = temp_generator(x)\n",
    "print(x.shape, result.shape, y.shape)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Batch Processing Implementation\n\nThe `compute_subset_sum_batch` function extends our differentiable computation to handle multiple examples simultaneously:\n\n- **Tensor Operations**: Uses broadcasting for efficient batch computation\n- **Gradient Flow**: Maintains full differentiability across the batch dimension\n- **Memory Efficiency**: Processes multiple subset sum problems in parallel\n\nThis batched implementation is essential for training neural networks on large datasets of subset sum instances.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 10\n",
    "# set_length = 5\n",
    "\n",
    "# input_shape = (batch_size, set_length)\n",
    "# output_shape = (batch_size, set_length, 2)\n",
    "# x = tf.random.normal(input_shape)\n",
    "# y = tf.math.round(tf.random.uniform(output_shape, minval=0, maxval=1))\n",
    "\n",
    "# temp_generator = CandidateGeneratorBlock(set_length)\n",
    "# a = Input(shape=(set_length))\n",
    "# b = temp_generator(a)\n",
    "# m = Model(inputs=a, outputs=b)\n",
    "# m.compile(loss='mse', optimizer='adam')\n",
    "# m.fit(x=x,y=y,epochs=100,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Training Dataset Generation\n\nWe create a synthetic dataset of subset sum problems:\n\n1. **Random Sets**: Generate random integer sets of fixed length\n2. **Valid Solutions**: Create random binary masks for each set\n3. **Target Computation**: Calculate the corresponding target sums\n4. **Data Format**: Concatenate sets and targets for network input\n\nThis synthetic approach allows us to train on a large variety of subset sum instances with known ground truth solutions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 0 1 0 1]\n",
      "  [0 1 0 1 0]]\n",
      "\n",
      " [[0 0 1 1 1]\n",
      "  [1 1 0 0 0]]]\n",
      "[16 4.30306244]\n",
      "[[0.268941402 0.731058598 0.268941402 0.731058598 0.268941402]\n",
      " [0.731058598 0.731058598 0.268941402 0.268941402 0.268941402]]\n",
      "[[[-1.76950729 1.76950753]\n",
      "  [-1.37628365 1.37628353]\n",
      "  [-0.786447704 0.786447763]\n",
      "  [-1.76950753 1.76950729]\n",
      "  [-0.589835823 0.589835823]]\n",
      "\n",
      " [[0 0]\n",
      "  [0 0]\n",
      "  [-1.37628353 1.37628365]\n",
      "  [-0.786447704 0.786447763]\n",
      "  [-0.983059645 0.983059704]]]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def compute_subset_sum_batch(S, M):\n",
    "    M = tf.transpose(M,  [0, 2, 1])\n",
    "    pos = tf.reduce_sum(S * M[:,1], axis=-1)\n",
    "    neg = tf.reduce_sum(S * (1 - M[:,0]), axis=-1)\n",
    "    return (pos + neg) / 2\n",
    "\n",
    "dataset_size = 2\n",
    "set_length = 5\n",
    "h_S = np.random.randint(0, 10, (dataset_size, set_length))\n",
    "h_M = np.random.randint(0, 2, (dataset_size, set_length))\n",
    "\n",
    "S = tf.Variable(h_S, dtype=tf.float32)\n",
    "M = tf.Variable(tf.one_hot(h_M, 2), dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    M_s = tf.nn.softmax(M, axis=-1)\n",
    "    T_ = compute_subset_sum_batch(S, M_s)\n",
    "\n",
    "tf.print(tf.transpose(M, [0, 2, 1]))\n",
    "tf.print(T_)\n",
    "tf.print(tape.gradient(T_, S))\n",
    "tf.print(tape.gradient(T_, M))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Training Results Analysis\n\nThe training output shows the neural network learning to generate increasingly accurate subset selections:\n\n1. **Early Training**: Network produces suboptimal candidates with high loss\n2. **Mid Training**: Gradual improvement in candidate quality and target matching\n3. **Late Training**: Convergence toward valid binary solutions with low error\n\nThe network learns to recognize patterns in subset sum problems and generate candidates that are close to optimal solutions, significantly reducing the search space for subsequent fine-tuning.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## Fine-Tuning Neural Network Candidates\n\nOnce the neural network generates an initial candidate $\\bar{M}$, we can **fine-tune** this candidate using our differentiable subset sum function. This two-stage approach combines the strengths of both neural networks and optimization:\n\n### Two-Stage Optimization Strategy\n\n1. **Stage 1 - Neural Generation**: Use the trained network to quickly generate a reasonable initial candidate\n2. **Stage 2 - Local Optimization**: Fine-tune the candidate using gradient descent with our subset sum and bistable losses\n\n### Benefits of This Approach\n\n- **Warm Start**: The neural network provides a much better initialization than random guessing\n- **Faster Convergence**: Starting near a good solution reduces optimization time\n- **Higher Success Rate**: Better initial candidates increase the likelihood of finding valid solutions\n- **Scalability**: The neural network can generalize to new problem instances\n\n### Implementation Details\n\nThe fine-tuning process uses the same loss combination as before:\n- **Primary Loss**: L2 distance between predicted and target sums\n- **Regularization**: Bistable loss to maintain binary constraints\n- **Optimization**: Adam optimizer for stable gradient updates\n\nThis hybrid approach demonstrates how neural networks can be integrated with differentiable algorithms to tackle computationally hard problems."
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000\n",
    "set_length = 5\n",
    "S = np.random.randint(0, 10, (dataset_size, set_length))\n",
    "M = np.random.randint(0, 2, (dataset_size, set_length))\n",
    "T = np.reshape(np.sum(S * M, axis=1), (dataset_size, 1))\n",
    "ST = np.concatenate((S, T), axis=1)\n",
    "\n",
    "S = np.float32(S)\n",
    "T = np.float32(T)\n",
    "ST = np.float32(ST)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Summary and Key Insights\n\nThis notebook demonstrates a **differentiable programming approach** to the NP-complete subset sum problem, showcasing several important techniques:\n\n### Technical Contributions\n\n1. **Differentiable Formulation**: Reformulating discrete optimization as continuous optimization with gradient-based methods\n2. **Bistable Loss Function**: Using specialized loss functions to encourage binary solutions while maintaining differentiability\n3. **One-Hot Softmax Encoding**: Structured representation that enforces mutual exclusivity constraints\n4. **Neural-Algorithmic Hybrid**: Combining neural networks for candidate generation with optimization-based fine-tuning\n\n### Key Insights\n\n- **Relaxation Strategy**: Continuous relaxation enables gradient-based optimization of discrete problems\n- **Regularization Importance**: Bistable loss is crucial for recovering binary solutions from continuous optimization\n- **Hybrid Approaches**: Neural networks excel at generating good initializations for traditional optimization\n- **End-to-End Learning**: The entire pipeline remains differentiable, enabling sophisticated training strategies\n\n### Applications and Extensions\n\nThis approach can be extended to other combinatorial optimization problems such as:\n- **Graph Coloring**: Using similar binary selection mechanisms\n- **Traveling Salesman Problem**: With appropriate sequence-based neural architectures\n- **Boolean Satisfiability**: Extending the boolean logic framework from earlier notebooks\n- **Resource Allocation**: In scheduling and planning domains\n\nThe techniques demonstrated here represent a general framework for making discrete optimization problems amenable to modern machine learning approaches while maintaining the expressiveness of classical algorithmic methods.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=36.61646>,\n",
       " <tf.Tensor: shape=(2, 5, 2), dtype=float32, numpy=\n",
       " array([[[9.99797642e-01, 2.02336319e-04],\n",
       "         [1.36642475e-05, 9.99986291e-01],\n",
       "         [5.92471451e-07, 9.99999404e-01],\n",
       "         [7.41534710e-01, 2.58465230e-01],\n",
       "         [9.99999762e-01, 2.91080596e-07]],\n",
       " \n",
       "        [[9.69633400e-01, 3.03665511e-02],\n",
       "         [2.41654180e-02, 9.75834608e-01],\n",
       "         [2.50900686e-01, 7.49099314e-01],\n",
       "         [9.10448074e-01, 8.95519182e-02],\n",
       "         [6.18431568e-01, 3.81568432e-01]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([14.068474,  8.259684], dtype=float32)>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step(S[:2], T[:2], ST[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.0997086 [0.000234175328 0.999985218 0.999999404 0.309308052 2.78606336e-07] 14.4753399 12 21\n",
      "10.1025267 [0.181596816 0.999999046 0.999754846 0.947611749 7.96055222e-09] 20.3053169 20 21\n",
      "6.48026657 [0.56387651 0.999999404 0.960552514 0.997461319 1.07887754e-09] 21.9196129 24 21\n",
      "5.73290539 [0.756967843 0.999997735 0.944468737 0.999148846 2.01228881e-10] 22.5768032 24 21\n",
      "5.04855633 [0.951817334 0.999989033 0.886824906 0.999789774 3.92590821e-11] 22.9001427 24 21\n",
      "4.72130775 [0.991167903 0.999964237 0.827799082 0.9999578 9.12365409e-12] 22.5865822 24 21\n",
      "4.49034357 [0.998867631 0.999913454 0.796109259 0.999991298 2.26091736e-12] 22.3639297 24 21\n",
      "4.30261469 [0.999887347 0.999844432 0.768945515 0.999997735 6.6921631e-13] 22.1504745 24 21\n",
      "4.15217876 [0.999986887 0.999717176 0.748130858 0.999998927 2.8707085e-13] 21.9838562 24 21\n",
      "4.01752472 [0.99999845 0.999705493 0.727931 0.999999404 1.01167677e-13] 21.822258 24 21\n",
      "[[1.5196631e-06 0.000294527214 0.272068977 5.36702942e-07 1]\n",
      " [0.99999845 0.999705493 0.727931 0.999999404 1.01167677e-13]]\n"
     ]
    }
   ],
   "source": [
    "s = tf.constant(S[0], dtype=tf.float32)\n",
    "s = tf.expand_dims(s, 0)\n",
    "\n",
    "for i in range(1000):\n",
    "    loss, m, t_ = train_step(S, T, ST)\n",
    "    if i % 100 == 0:\n",
    "        m = m[0]\n",
    "        t_ = t_[0]\n",
    "        m_t = tf.transpose(m)\n",
    "        r_m = tf.round(m)\n",
    "        r_m = tf.expand_dims(r_m, 0)\n",
    "        actual = compute_subset_sum_batch(s, r_m)[0]\n",
    "        tf.print(loss, m_t[1], t_, actual, int(T[0,0]))\n",
    "        \n",
    "tf.print(m_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 1.],\n",
       "       [1., 1., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nS = np.array([[1, 2, 3, 4, 5]], dtype=np.float32)\n",
    "nT = np.array([[7]], dtype=np.float32)\n",
    "nST = np.concatenate((nS, nT), axis=1)\n",
    "nM = generator(nST)\n",
    "np.round(nM)[0].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning the candidate\n",
    "\n",
    "Once the neural network has guessed the initial candidate $\\bar{M}$, we can now fine tune it using `compute_subset_sum` function. Assuming the neural network has learnt a good representation of the problem, the candidate should equal or at least be close to the solution. Therefore, we could expect it to converge faster and more reliably than manually guessing a candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25990248 [0.723222 0.650265336 0.291595429 0.729313731 0.268548429] 7.15885067 7\n",
      "3.55748272 [0.76129365 0.69552058 0.252263904 0.766952276 0.231619418] 7.13519096 7\n",
      "2.89820313 [0.794480681 0.738415241 0.217733368 0.799760461 0.199965134] 7.12346792 7\n",
      "2.3320303 [0.822267354 0.775921702 0.188515246 0.82707417 0.173519135] 7.11562 7\n",
      "1.87315917 [0.845076859 0.807170928 0.164326832 0.84936887 0.151737064] 7.10862875 7\n",
      "1.51269031 [0.863694 0.832624137 0.144478917 0.867491841 0.133874401] 7.10178614 7\n",
      "1.2330004 [0.878932178 0.853232145 0.128189027 0.882288456 0.119186744] 7.09511614 7\n",
      "1.01610804 [0.89149785 0.869977891 0.114743605 0.894473612 0.107026093] 7.08877087 7\n",
      "0.846875072 [0.901959956 0.883702695 0.103550903 0.904613614 0.0968662798] 7.08286 7\n",
      "0.713556945 [0.910761774 0.895073354 0.0941422805 0.913143694 0.0882939249] 7.07743168 7\n",
      "[[0.0892381892 0.104926601 0.905857742 0.0868563354 0.91170609]\n",
      " [0.910761774 0.895073354 0.0941422805 0.913143694 0.0882939249]]\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def fine_tune_step(S, M, T):\n",
    "    with tf.GradientTape() as tape:\n",
    "        M_s = tf.nn.softmax(M, axis=1)\n",
    "        T_ = compute_subset_sum_v2(S, M_s)\n",
    "        loss = tf.nn.l2_loss(T_ - T)\n",
    "        loss += tf.reduce_sum(bistable_loss(M_s)) * 10\n",
    "    \n",
    "    grads = tape.gradient(loss, M)\n",
    "    opt.apply_gradients(zip([grads], [M]))\n",
    "    \n",
    "    return loss, T_\n",
    "\n",
    "S = tf.Variable(nS,dtype=tf.float32)\n",
    "M = tf.Variable(nM[0], dtype=tf.float32)\n",
    "T = 7\n",
    "\n",
    "for i in range(1000):\n",
    "    loss, T_ = fine_tune_step(S, M, T)\n",
    "    if i % 100 == 0:\n",
    "        M_T = tf.transpose(M)\n",
    "        M_T = tf.nn.softmax(M_T, axis=0)\n",
    "        M_s = tf.nn.softmax(M, axis=1)\n",
    "        actual = compute_subset_sum_v2(S, tf.round(M_s))\n",
    "        tf.print(loss, M_T[1], T_[0], actual[0])\n",
    "        \n",
    "tf.print(M_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
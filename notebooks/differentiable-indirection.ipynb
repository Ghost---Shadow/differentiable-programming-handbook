{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Differentiable Indirection\n\nIndirection is a fundamental concept in computer programming where we access data through references, pointers, or containers rather than directly. This notebook explores how to make indirection operations differentiable, enabling gradient-based optimization of data structures and algorithms.\n\n## Understanding Indirection\n\nIn traditional programming, [indirection](https://en.wikipedia.org/wiki/Indirection) (also called dereferencing) allows us to:\n- Access memory through pointers\n- Navigate data structures like linked lists and trees\n- Implement dynamic dispatch and virtual function calls\n- Create flexible, indirect addressing schemes\n\n## Differentiable Indirection Strategies\n\nContinuous and differentiable indirection can be categorized into two main approaches:\n\n1. **Data-Driven Addressing**: The data itself determines how it should be accessed\n   - Example: [Transformers](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) where query and key vectors generate attention weights\n   - The addressing mechanism emerges from the data content\n\n2. **External Addressing**: Addressing patterns are learned independently of the data\n   - Example: [Neural Turing Machines](https://arxiv.org/pdf/1410.5401.pdf) with separate addressing mechanisms\n   - The controller learns how to navigate memory structures\n\n## Case Study: Differentiable Linked Lists\n\nWe'll implement a differentiable circular linked list as our primary example, demonstrating how classic pointer-based data structures can be made trainable."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Differentiable Linked Lists\n\nA [linked list](https://en.wikipedia.org/wiki/Linked_list) is a fundamental data structure where elements are connected through pointers. We'll implement a circular linked list where the last element points back to the first.\n\n![circular linked list](./images/525px-Circularly-linked-list.svg.png)\n\n### Traditional Linked List Structure\n\nA linked list consists of:\n- **Data elements**: The actual values stored in the list\n- **Pointers**: References to the next element in the sequence\n\n### Challenge: Making Pointers Differentiable\n\nTraditional pointers are discrete indices that break differentiability. We need to replace discrete pointer operations with continuous, differentiable alternatives while preserving the linked list semantics."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## From Discrete to Continuous Representation\n\n### Memory as Indexable Arrays\n\nWe can model computer memory as two parallel arrays:\n- **Data array**: Contains the actual values\n- **Pointer array**: Contains indices to the next elements\n\n**Example**: A linked list with data `[12, 37, 99]` and traversal order `12 → 99 → 37 → 12`\n\n$$\\text{data} = [12, 37, 99]$$\n$$\\text{pointers}_{\\text{next}} = [2, 0, 1]$$\n\n### Traditional Traversal\n\nThe pointer array defines the traversal order:\n- `pointers_next[0] = 2` → from element 0, go to element 2\n- `pointers_next[2] = 1` → from element 2, go to element 1  \n- `pointers_next[1] = 0` → from element 1, go back to element 0 (circular)\n\n**Traditional traversal code**:\n```python\nptr = 0\nfor _ in range(3):\n    print(data[ptr])        # Print current data\n    ptr = pointers_next[ptr] # Move to next element\n```\n\n**Problem**: This uses discrete indexing operations that prevent gradient flow and make the structure non-trainable."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Superposition-Based Differentiable Pointers\n\nWe solve the differentiability problem using **superposition lookup** (as detailed in [differentiable-indexed-arrays.ipynb](differentiable-indexed-arrays.ipynb)).\n\n### One-Hot Pointer Representation\n\nInstead of discrete indices, we use one-hot vectors as \"soft pointers\":\n\n**Current position**: $\\mathbf{p} = [1, 0, 0]$ (pointing to element 0)\n\n**Data lookup**: $\\text{element}_i = \\mathbf{data} \\cdot \\mathbf{p}$\n\n$$\\text{element}_i = \\begin{bmatrix} 12 & 37 & 99 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix} = 12$$\n\n### Soft Pointer Lookup\n\nThe beauty of superposition is that we can have \"partial\" pointers:\n\n$$\\text{element}_i = \\begin{bmatrix} 12 & 37 & 99 \\end{bmatrix} \\begin{bmatrix} 0.5 \\\\ 0 \\\\ 0.5 \\end{bmatrix} = 55.5$$\n\nThis gives us a blend of elements 0 and 2, maintaining differentiability!\n\n### Pointer Transition Matrix\n\nWe convert the discrete pointer array to a **transition matrix** $\\mathbf{P}$:\n\n**Discrete pointers**: $\\text{pointers}_{\\text{next}} = [2, 0, 1]$\n\n**Transition matrix**: \n$$\\mathbf{P} = \\begin{bmatrix}\n0 & 0 & 1 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{bmatrix}$$\n\n### Differentiable Traversal\n\nNavigation becomes matrix multiplication:\n$$\\mathbf{p}_{i+1} = \\mathbf{p}_i \\mathbf{P}$$\n\n**Example**:\n$$\\mathbf{p}_1 = \\mathbf{p}_0 \\mathbf{P} = \\begin{bmatrix} 1 & 0 & 0 \\end{bmatrix} \\begin{bmatrix} 0 & 0 & 1 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix} = \\begin{bmatrix} 0 & 0 & 1 \\end{bmatrix}$$\n\nThis smoothly transitions from position 0 to position 2, exactly as intended, but with full differentiability!"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Implementation\n\nLet's implement the differentiable linked list traversal:",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Learning Problem: Discovering Optimal Traversal Orders\n\nNow for the exciting part: **can we learn the optimal pointer structure?**\n\n### Problem Formulation\n\nGiven:\n- **Data array**: `[1, 3, 2, 5, 4]` (unordered)\n- **Target sequence**: `[1, 2, 3, 4, 5]` (sorted order)\n\n**Find**: A transition matrix $\\mathbf{P}$ such that traversing the linked list produces the target sequence.\n\n### Key Insight\n\nWe're not looking for a standard permutation matrix that transforms `data → target` directly. Instead, we want:\n\n$$y_i = \\mathbf{p}_0 \\mathbf{P}^i \\mathbf{x}$$\n\nwhere $\\mathbf{p}_0 = [1, 0, 0, \\ldots, 0]$ is the starting position.\n\n### Circular Property\n\nFor circular linked lists, the transition matrix must satisfy:\n$$\\mathbf{P}^n = \\mathbf{I}$$\n\nwhere $n$ is the cycle length. This ensures that after $n$ steps, we return to the starting configuration.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([1. 1. 1.], shape=(3,), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[3. 4. 5.]\n",
      " [0. 0. 0.]\n",
      " [1. 3. 2.]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def iterate_over(data, nexts):\n",
    "    data_len = tf.shape(data)[0]\n",
    "    P = nexts\n",
    "    p = tf.one_hot([0], data_len)\n",
    "    \n",
    "    x = tf.expand_dims(data, -1)\n",
    "    y_ = tf.zeros((data_len))\n",
    "    eye = tf.eye(data_len)\n",
    "    \n",
    "    for i in tf.range(data_len):\n",
    "        # The @ token denotes matrix multiplication\n",
    "        x_scalar = tf.squeeze(p @ x)\n",
    "        y_ += eye[i] * x_scalar\n",
    "        \n",
    "        p = p @ P\n",
    "\n",
    "    return y_\n",
    "\n",
    "data  = tf.Variable([1, 3, 2], dtype=tf.float32)\n",
    "target = tf.Variable([1, 2, 3], dtype=tf.float32)\n",
    "data_len = tf.shape(data)[0]\n",
    "nexts = tf.Variable(tf.one_hot([2, 0, 1], data_len), dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    result = iterate_over(data, nexts)\n",
    "    loss = tf.nn.l2_loss(result - target)\n",
    "    \n",
    "print(result)\n",
    "print(tape.gradient(result, data))\n",
    "print(tape.gradient(result, nexts))\n",
    "\n",
    "print(loss)\n",
    "print(tape.gradient(loss, nexts))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Multi-Constraint Loss Function\n\nLearning a valid permutation matrix requires careful constraint design. We need the learned matrix $\\mathbf{P}$ to satisfy multiple properties simultaneously.\n\n### Loss Components\n\nOur total loss combines several constraints:\n\n$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{task}} + \\mathcal{L}_{\\text{permutation}}$$\n\n**Task Loss**: Sequence matching\n$$\\mathcal{L}_{\\text{task}} = ||\\mathbf{y}_{\\text{pred}} - \\mathbf{y}_{\\text{target}}||^2$$\n\n**Permutation Loss**: Ensures $\\mathbf{P}$ is a valid permutation matrix\n$$\\mathcal{L}_{\\text{permutation}} = \\mathcal{L}_{\\text{row}} + \\mathcal{L}_{\\text{col}} + \\mathcal{L}_{\\text{bistable}} + \\mathcal{L}_{\\text{cycle}}$$\n\n### Permutation Matrix Constraints\n\n1. **Row Constraint**: Each row sums to 1 (exactly one outgoing edge per node)\n2. **Column Constraint**: Each column sums to 1 (exactly one incoming edge per node)  \n3. **Bistable Constraint**: Elements should be close to 0 or 1 (discrete behavior)\n4. **Cycle Constraint**: $\\mathbf{P}^n = \\mathbf{I}$ (returns to start after full cycle)\n\nThese constraints together ensure that $\\mathbf{P}$ represents a valid permutation while allowing gradient-based optimization.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Bistable Loss Function\n\nThe bistable loss encourages matrix elements to be either 0 or 1, preventing soft intermediate values. More details about this loss function can be found in the [boolean-satisfiability.ipynb](boolean-satisfiability.ipynb) notebook."
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = tf.one_hot([2, 4, 1, 0, 3], 5)\n",
    "Q @ Q @ Q @ Q @ Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "We want the predicted $\\bar{y}$ to match the real $y$ after one complete traversal of the linked list. So, we add an L2 loss $ | \\bar{y} - y | $. However, this causes the network to learn a $P$ matrix which is a linear combination of the $x$ instead of learning a permutation matrix. Thus we need to add more losses to make sure that the $P$ matrix is a permutation matrix.\n",
    "\n",
    "For permutation matrix loss, we have the following rules\n",
    "* All columns must add up to 1\n",
    "* All rows must add up to 1\n",
    "* All elements must be close to either 0 or 1 (bistable loss)\n",
    "* Cycle loss $P^n = I$ (if cyclic)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Training the Differentiable Linked List\n\nNow we train our system to discover the correct traversal order!\n\n### Critical Implementation Details\n\n1. **Softmax Normalization**: Before each forward pass, we apply softmax to the transition matrix:\n   ```python\n   P_normalized = tf.nn.softmax(P, axis=1)\n   ```\n   This ensures row-wise probability distributions while maintaining differentiability.\n\n2. **Initialization Strategy**: Counterintuitively, starting with an *invalid* permutation matrix often leads to faster convergence than starting with a valid one. This is because:\n   - Invalid initializations have higher initial gradients\n   - The optimization landscape may be smoother from invalid starting points\n   - Random noise helps escape local minima\n\n3. **Defuzzification Check**: We monitor both the soft result and a \"defuzzified\" version where we:\n   - Take `argmax` of each row to get discrete indices\n   - Reconstruct a hard permutation matrix\n   - Verify it produces the same traversal order\n\nThis ensures our learned soft matrix corresponds to a valid discrete permutation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information about `bistable_loss` can be found [here](notebooks/boolean-satisfiability.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Verifying the Circular Property\n\nLet's verify that our learned matrix satisfies the circular property $\\mathbf{P}^n = \\mathbf{I}$, which is essential for circular linked lists.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from library.loss import bistable_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Circular Property Verification\n\nPerfect! Both the soft (continuous) and defuzzified (discrete) versions of our learned matrix satisfy the circular property:\n- **Soft matrix**: $\\mathbf{P}^5 \\approx \\mathbf{I}$ (rounded to identity)\n- **Discrete matrix**: $\\mathbf{D}^5 = \\mathbf{I}$ (exactly identity)\n\nThis confirms that our differentiable linked list correctly implements circular traversal semantics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "tf.Tensor(13.0, shape=(), dtype=float32)\n",
      "tf.Tensor(2.9646, shape=(), dtype=float32)\n",
      "tf.Tensor(0.75, shape=(), dtype=float32)\n",
      "tf.Tensor(2.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def permute_matrix_loss(P, cycle_length=1, cycle_weight=0):\n",
    "    loss = 0\n",
    "    \n",
    "    P_square = tf.math.square(P)\n",
    "    axis_1_sum = tf.reduce_sum(P_square, axis=1)\n",
    "    axis_0_sum = tf.reduce_sum(P_square, axis=0)\n",
    "    \n",
    "    # Penalize axes not adding up to one\n",
    "    loss += tf.nn.l2_loss(axis_1_sum - 1)\n",
    "    loss += tf.nn.l2_loss(axis_0_sum - 1)\n",
    "    \n",
    "    # Penalize numbers outside [0, 1]\n",
    "    loss += tf.math.reduce_sum(bistable_loss(P))\n",
    "    \n",
    "    # Cycle loss\n",
    "    Q = P\n",
    "    for _ in tf.range(cycle_length - 1):\n",
    "        Q = P @ Q\n",
    "    cycle_loss = tf.nn.l2_loss(Q - tf.eye(tf.shape(Q)[0]))\n",
    "    loss += cycle_loss * cycle_weight\n",
    "    \n",
    "    return loss\n",
    "\n",
    "test1 = tf.constant([\n",
    "    [1,0,0],\n",
    "    [0,1,0],\n",
    "    [0,0,1]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "test2 = tf.constant([\n",
    "    [0,1,0],\n",
    "    [1,0,0],\n",
    "    [0,0,1]\n",
    "],dtype=tf.float32)\n",
    "\n",
    "test3 = tf.constant([\n",
    "    [-1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "],dtype=tf.float32)\n",
    "\n",
    "test4 = tf.constant([\n",
    "    [2, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1],\n",
    "],dtype=tf.float32)\n",
    "\n",
    "test5 = tf.constant([\n",
    "    [0.1, 0, 0],\n",
    "    [0, 0.1, 0],\n",
    "    [0, 0, 0.1],\n",
    "],dtype=tf.float32)\n",
    "\n",
    "test6 = tf.constant([\n",
    "    [0.5, 0.5, 0],\n",
    "    [0.5, 0.5, 0],\n",
    "    [0, 0, 1],\n",
    "],dtype=tf.float32)\n",
    "\n",
    "test7 = tf.constant([\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 1],\n",
    "],dtype=tf.float32)\n",
    "\n",
    "print(permute_matrix_loss(test1))\n",
    "print(permute_matrix_loss(test2))\n",
    "print(permute_matrix_loss(test3))\n",
    "print(permute_matrix_loss(test4))\n",
    "print(permute_matrix_loss(test5))\n",
    "print(permute_matrix_loss(test6))\n",
    "print(permute_matrix_loss(test7, 1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Summary and Applications\n\nWe successfully implemented and trained a differentiable linked list that:\n\n### Key Achievements\n\n1. ✅ **Preserved Semantics**: Maintains linked list traversal behavior\n2. ✅ **Full Differentiability**: Enables gradient-based optimization\n3. ✅ **Learned Structure**: Automatically discovered optimal pointer configuration\n4. ✅ **Circular Properties**: Correctly implements circular traversal\n5. ✅ **Discrete Convergence**: Soft matrices converge to valid permutations\n\n### Technical Innovations\n\n- **Superposition Pointers**: Replace discrete indices with probability distributions\n- **Matrix Multiplication Traversal**: Navigation through matrix operations\n- **Multi-Constraint Training**: Simultaneous task and structure learning\n- **Soft-to-Hard Convergence**: Continuous optimization yielding discrete structures\n\n### Broader Applications\n\nThis differentiable indirection framework enables:\n\n- **Neural Data Structures**: Trainable stacks, queues, trees, graphs\n- **Learnable Algorithms**: Sorting, searching, graph traversal algorithms  \n- **Memory-Augmented Networks**: Neural Turing Machines, Differentiable Neural Computers\n- **Attention Mechanisms**: Transformer architectures with learned addressing\n- **Program Synthesis**: Learning program control flow and data access patterns\n\n### Implications\n\nDifferentiable indirection bridges the gap between:\n- **Traditional algorithms** (discrete, exact, non-trainable)\n- **Neural networks** (continuous, approximate, trainable)\n\nThis opens new possibilities for **end-to-end learning** of complete algorithmic systems, where both the computation and the data access patterns can be optimized simultaneously through gradient descent.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "For convergence, a [softmax](https://en.wikipedia.org/wiki/Softmax_function) operation on $P$ is critical before traversing and computing loss. `TODO: Why?`\n",
    "\n",
    "Initializing $P$ with any invalid permutation matrix leads to a faster convergence than valid permutation matrix. `TODO: WHY?`\n",
    "\n",
    "In order to make sure that our matrix $P$ is learning a permutation matrix and not linear combination of the input, we also print a defuzzified result `y_defuzz`. It is generated by taking the `argmax` of $P$ and iterating over it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   loss  |   y_pred  | y_defuzz |   P_pred  |   P_actual   |\n",
      "8.7374239 [1 3 3 3 3] [1 3 3 3 3] [1 1 1 1 1] [2, 4, 1, 0, 3]\n",
      "5.66760302 [1 2 3 4 4] [1 2 3 5 5] [2 3 1 3 3] [2, 4, 1, 0, 3]\n",
      "3.87311959 [1 2 3 4 4] [1 2 3 4 5] [2 4 1 3 3] [2, 4, 1, 0, 3]\n",
      "1.50285113 [1 2 3 4 5] [1 2 3 4 5] [2 4 1 0 3] [2, 4, 1, 0, 3]\n",
      "0.205469772 [1 2 3 4 5] [1 2 3 4 5] [2 4 1 0 3] [2, 4, 1, 0, 3]\n",
      "0.0740788057 [1 2 3 4 5] [1 2 3 4 5] [2 4 1 0 3] [2, 4, 1, 0, 3]\n",
      "0.0350763313 [1 2 3 4 5] [1 2 3 4 5] [2 4 1 0 3] [2, 4, 1, 0, 3]\n",
      "0.0185657572 [1 2 3 4 5] [1 2 3 4 5] [2 4 1 0 3] [2, 4, 1, 0, 3]\n",
      "0.0103952968 [1 2 3 4 5] [1 2 3 4 5] [2 4 1 0 3] [2, 4, 1, 0, 3]\n",
      "0.00600781338 [1 2 3 4 5] [1 2 3 4 5] [2 4 1 0 3] [2, 4, 1, 0, 3]\n",
      "[[-2.18890929 -2.56413817 4.51624966 -2.87268877 -3.21818304]\n",
      " [-2.38402414 -2.36196756 -2.74159765 -1.29020119 4.7228055]\n",
      " [-2.63246131 5.06587076 -2.92895341 -2.58874464 -2.98058629]\n",
      " [3.2075038 -3.05285668 -3.33832383 -1.79198897 -2.805336]\n",
      " [-2.95142794 -2.45104289 -3.21477652 4.53860283 -2.06163]]\n"
     ]
    }
   ],
   "source": [
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "@tf.function\n",
    "def train_step(data, nexts, target_data):\n",
    "    data_length = tf.shape(data)[0]\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        nextss = tf.nn.softmax(nexts, axis=1)\n",
    "        actual_data = iterate_over(data, nextss)\n",
    "        loss = tf.nn.l2_loss(actual_data - target_data)\n",
    "        loss += permute_matrix_loss(nextss, data_length, 1)\n",
    "    \n",
    "    grads = tape.gradient(loss, nexts)\n",
    "    opt.apply_gradients(zip([grads], [nexts]))\n",
    "    \n",
    "    return loss, actual_data\n",
    "\n",
    "data  = tf.constant([1, 3, 2, 5, 4], dtype=tf.float32)\n",
    "target_data  = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32)\n",
    "data_len = tf.shape(data)[0]\n",
    "# nexts = tf.Variable(tf.one_hot([2, 4, 1, 0, 3], data_len), dtype=tf.float32)\n",
    "nexts = tf.Variable(tf.one_hot([1, 1, 1, 1, 1], data_len), dtype=tf.float32)\n",
    "# nexts = tf.Variable(tf.one_hot([0,0,0,0,0], data_len), dtype=tf.float32)\n",
    "# nexts = tf.Variable(tf.one_hot([4, 4, 4, 4, 4], data_len), dtype=tf.float32)\n",
    "# nexts = tf.Variable(tf.one_hot([3, 3, 3, 3, 3], data_len), dtype=tf.float32)\n",
    "# nexts = tf.Variable(tf.one_hot([1, 2, 3, 4, 5], data_len), dtype=tf.float32)\n",
    "# nexts = tf.Variable(tf.random.uniform((data_len, data_len), 0, 1))\n",
    "\n",
    "tf.print('|   loss  |   y_pred  | y_defuzz |   P_pred  |   P_actual   |')\n",
    "for i in range(10000):\n",
    "    loss, actual_data = train_step(data, nexts, target_data)\n",
    "    if i % 1000 == 0:\n",
    "        argmax_next = tf.argmax(nexts, 1)\n",
    "        defuzzified = tf.one_hot(argmax_next, data_len)\n",
    "        defuzzified_data = iterate_over(data, defuzzified)\n",
    "        tf.print(loss, tf.round(actual_data), defuzzified_data, argmax_next, [2, 4, 1, 0, 3])\n",
    "        \n",
    "tf.print(nexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying cyclic permutation\n",
    "\n",
    "We can see that $P^n = I$ for both normal and defuzzified cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 4 1 0 3]\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = tf.nn.softmax(nexts, axis=1)\n",
    "tf.print(tf.argmax(P,1))\n",
    "tf.round(P @ P @ P @ P @ P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=float32, numpy=\n",
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax_next = tf.argmax(P, 1)\n",
    "DQ = tf.one_hot(argmax_next, data_len)\n",
    "DQ @ DQ @ DQ @ DQ @ DQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
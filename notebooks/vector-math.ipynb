{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Differentiable Vector Operations\n\nVector mathematics forms the foundation of many differentiable programming applications. This notebook explores how to implement **differentiable vector transformations** that maintain gradient flow while performing discrete-like operations such as permutations and shifts.\n\n## The Challenge of Discrete Vector Operations\n\nTraditional vector operations like shifting, rotating, or permuting elements are inherently discrete:\n- **Hard indexing**: Elements are moved from one position to another\n- **Non-differentiable**: Standard implementations don't preserve gradients\n- **Combinatorial nature**: Many vector operations involve discrete choices\n\nMaking these operations differentiable requires careful reformulation using **linear algebraic techniques** that preserve both the intended semantics and gradient information.\n\n## Applications in Differentiable Programming\n\nDifferentiable vector operations enable:\n- **Learnable data structures**: Arrays and sequences that can be optimized end-to-end\n- **Neural program synthesis**: Learning algorithms that manipulate vector data\n- **Attention mechanisms**: Differentiable addressing and memory access patterns\n- **Sequence modeling**: Learnable transformations of sequential data",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Summary: Principles of Differentiable Vector Mathematics\n\nThis notebook demonstrates a fundamental technique for making discrete vector operations differentiable through **linear algebraic reformulation**.\n\n### Core Innovation: Matrix-Based Permutations\n\nInstead of using discrete indexing operations that break gradient flow, we:\n1. **Represent operations as matrices**: Use permutation matrices to encode shifts/rotations\n2. **Apply via matrix multiplication**: Leverage the linearity of matrix operations\n3. **Preserve gradients**: Maintain differentiability throughout the transformation\n4. **Generalize easily**: Extend to arbitrary permutations and transformations\n\n### Technical Insights\n\n- **Linearity preservation**: Matrix multiplication maintains gradient flow\n- **Discrete semantics**: Achieves the same results as traditional indexing\n- **Uniform gradients**: Equal contribution from all input elements after transformation\n- **Composability**: Operations can be chained and combined\n\n### Applications and Extensions\n\nThis approach enables numerous differentiable programming applications:\n\n#### Data Structure Operations\n- **Differentiable arrays**: Learnable indexing and slicing operations\n- **Sorting networks**: Gradient-based learning of comparison functions\n- **Queue operations**: Differentiable push/pop operations for stacks and queues\n\n#### Sequence Processing\n- **Attention mechanisms**: Differentiable addressing in memory systems\n- **Sequence transformations**: Learnable reordering and alignment operations\n- **Time series analysis**: Gradient-based lag and lead operations\n\n#### Neural Architecture Components\n- **Permutation layers**: Learnable permutation operations in neural networks\n- **Routing operations**: Differentiable routing of information between layers\n- **Memory addressing**: Soft addressing mechanisms for external memory\n\n### Connection to Other Concepts\n\nThis vector math approach connects to several other differentiable programming techniques:\n\n- **Soft attention**: Similar to how attention creates differentiable addressing\n- **Differentiable data structures**: Forms the basis for learnable arrays and sequences\n- **Permutation learning**: Can be extended to learn arbitrary permutation matrices\n- **Linear algebra**: Demonstrates the power of matrix operations in differentiable programming\n\n### Mathematical Generalization\n\nThe permutation matrix approach can be generalized to:\n\n$$\\mathbf{v}_{transformed} = \\mathbf{v} \\cdot \\mathbf{T}$$\n\nWhere $\\mathbf{T}$ can be:\n- **Permutation matrices**: For discrete rearrangements\n- **Rotation matrices**: For continuous transformations  \n- **Learned matrices**: For application-specific transformations\n- **Stochastic matrices**: For probabilistic operations\n\n### Practical Considerations\n\nWhen implementing differentiable vector operations:\n- **Memory efficiency**: Matrix operations can be memory-intensive for large vectors\n- **Computational cost**: Matrix multiplication adds computational overhead\n- **Numerical stability**: Ensure matrices are well-conditioned\n- **Gradient flow**: Verify that gradients propagate correctly through transformations\n\nThis simple example illustrates the **power of linear algebra** in differentiable programming - by reformulating discrete operations as continuous matrix operations, we can maintain both semantic correctness and gradient flow, enabling end-to-end learning in complex systems.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]] [1 1 1]\n",
      "[[0 0.5 0.5]] [1 1 1]\n",
      "[[1 0 0]] [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "@tf.function\n",
    "def shift_left_one_hot(vec, shift=-1):\n",
    "    P = tf.eye(tf.shape(vec)[0])\n",
    "    P = tf.roll(P, shift=shift, axis=0)\n",
    "    \n",
    "    vec = tf.expand_dims(vec, 0)\n",
    "    \n",
    "    return vec @ P\n",
    "\n",
    "vec1 = tf.Variable([0,1,0], dtype=tf.float32)\n",
    "vec2 = tf.Variable([0.5,0.5,0], dtype=tf.float32)\n",
    "vec3 = tf.Variable([0,0,1], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    nv1 = shift_left_one_hot(vec1)\n",
    "    nv2 = shift_left_one_hot(vec2)\n",
    "    nv3 = shift_left_one_hot(vec3)\n",
    "\n",
    "tf.print(nv1, tape.gradient(nv1, vec1))\n",
    "tf.print(nv2, tape.gradient(nv2, vec2))\n",
    "tf.print(nv3, tape.gradient(nv3, vec3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Transformer Architecture in Differentiable Programming\n\nThe **Transformer** is a revolutionary neural network architecture introduced in \"Attention Is All You Need\" (Vaswani et al., 2017). It has become the foundation for modern language models and demonstrates key principles of differentiable programming through its entirely attention-based design.\n\n## Core Innovation: Self-Attention\n\nUnlike recurrent architectures, Transformers process sequences in parallel using **self-attention mechanisms**. This makes them particularly suitable for differentiable programming applications where we need:\n\n- **Parallel computation** across sequence elements\n- **Long-range dependencies** without gradient vanishing\n- **Interpretable attention patterns** for understanding model behavior\n- **Modular, composable components** that can be differentiated end-to-end\n\nThis implementation follows the [TensorFlow Transformer tutorial](https://www.tensorflow.org/tutorials/text/transformer) and demonstrates how complex neural architectures can be constructed using differentiable building blocks."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "    return pos * angle_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Positional Encoding\n\nSince Transformers lack recurrent connections, they need an explicit way to encode positional information. **Positional encoding** uses sinusoidal functions to create unique, learnable position representations.\n\n### Mathematical Foundation\n\nThe positional encoding uses alternating sine and cosine functions:\n\n- **Even positions**: $PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$\n- **Odd positions**: $PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$\n\nWhere:\n- $pos$ is the position in the sequence\n- $i$ is the dimension index\n- $d_{model}$ is the model dimension\n\nThis encoding allows the model to learn relative positions between tokens while maintaining differentiability.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Scaled Dot-Product Attention\n\nThe core of the Transformer is the **scaled dot-product attention** mechanism. This function computes attention weights between queries, keys, and values, enabling the model to focus on relevant parts of the input sequence.\n\n### Mathematical Formulation\n\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n\nWhere:\n- $Q$ (queries): What information we're looking for\n- $K$ (keys): What information is available\n- $V$ (values): The actual information content\n- $d_k$ is the key dimension (for scaling)\n\n### Key Components\n\n1. **Dot Product**: $QK^T$ computes similarity between queries and keys\n2. **Scaling**: Division by $\\sqrt{d_k}$ prevents extremely large values that could saturate softmax\n3. **Masking**: Optional masking prevents attention to certain positions (padding, future tokens)\n4. **Softmax**: Normalizes attention weights to sum to 1\n5. **Weighted Sum**: Attention weights are applied to values\n\nThis mechanism is fully differentiable and allows gradients to flow based on attention patterns.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Attention Example\n\nThis example demonstrates how attention works with concrete values:\n\n- **Keys**: One-hot vectors identifying different positions\n- **Values**: Information content at each position  \n- **Query**: Selects which key to attend to\n\nThe query `[0, 10, 0]` perfectly matches the second key `[0, 10, 0]`, resulting in:\n- **Attention weight**: 1.0 for the second position, 0.0 for others\n- **Output**: The second value `[10, 0]` is returned exactly\n\nThis shows how attention creates **content-based addressing** - the model can learn to look up information based on learned similarity patterns.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Multi-Head Attention\n\n**Multi-head attention** runs multiple attention functions in parallel, allowing the model to attend to different types of information simultaneously.\n\n### Architecture Design\n\nThe multi-head mechanism:\n\n1. **Linear Projections**: Projects Q, K, V through learned linear transformations\n2. **Head Splitting**: Divides the model dimension across multiple attention heads\n3. **Parallel Processing**: Runs scaled dot-product attention on each head independently\n4. **Concatenation**: Combines outputs from all heads\n5. **Final Projection**: Projects the concatenated result back to model dimension\n\n### Mathematical Formulation\n\n$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$\n\nWhere each head is:\n$$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$\n\n### Benefits for Differentiable Programming\n\n- **Diverse Representations**: Different heads can learn different types of relationships\n- **Parallelizability**: All heads compute independently, enabling efficient parallel processing\n- **Gradient Flow**: Multiple pathways for gradient information\n- **Interpretability**: Different heads often specialize in different linguistic or logical patterns",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "    q, k, v must have matching leading dimensions.\n",
    "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "    The mask has different shapes depending on its type(padding or look ahead) \n",
    "    but it must be broadcastable for addition.\n",
    "\n",
    "    Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable \n",
    "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "    output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Position-wise Feed-Forward Networks\n\nEach Transformer layer includes a **position-wise feed-forward network** that processes each position independently. This component adds non-linear transformation capabilities to the otherwise linear attention operations.\n\n### Architecture\n\nThe feed-forward network consists of:\n1. **First Linear Layer**: Expands dimensionality (typically 4x model dimension)\n2. **ReLU Activation**: Introduces non-linearity\n3. **Second Linear Layer**: Projects back to model dimension\n\n### Mathematical Representation\n\n$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n\n### Role in Differentiable Programming\n\n- **Non-linearity**: Essential for learning complex mappings\n- **Position Independence**: Each sequence position is processed identically\n- **Capacity**: The expansion to higher dimensions provides modeling capacity\n- **Regularization**: Can be augmented with dropout for better generalization",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights are:\n",
      "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
      "Output is:\n",
      "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def print_out(q, k, v):\n",
    "    temp_out, temp_attn = scaled_dot_product_attention(\n",
    "      q, k, v, None)\n",
    "    print ('Attention weights are:')\n",
    "    print (temp_attn)\n",
    "    print ('Output is:')\n",
    "    print (temp_out)\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "temp_k = tf.constant([[10,0,0],\n",
    "                      [0,10,0],\n",
    "                      [0,0,10],\n",
    "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
    "\n",
    "temp_v = tf.constant([[   1,0],\n",
    "                      [  10,0],\n",
    "                      [ 100,5],\n",
    "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
    "print_out(temp_q, temp_k, temp_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Encoder Layer\n\nThe **Encoder Layer** combines multi-head attention and feed-forward networks with crucial architectural innovations: **residual connections** and **layer normalization**.\n\n### Layer Architecture\n\nEach encoder layer follows this pattern:\n1. **Multi-Head Self-Attention**: Allows positions to attend to each other\n2. **Residual Connection + Dropout**: Adds input to attention output\n3. **Layer Normalization**: Normalizes the combined result\n4. **Feed-Forward Network**: Applies position-wise transformations\n5. **Residual Connection + Dropout**: Adds pre-FFN input to FFN output  \n6. **Layer Normalization**: Final normalization\n\n### Residual Connections\n\nThe residual connections are crucial for deep networks:\n$$\\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n\nBenefits:\n- **Gradient Flow**: Prevents vanishing gradients in deep networks\n- **Training Stability**: Easier optimization of deep architectures\n- **Identity Preservation**: Allows layers to learn incremental changes\n\n### Layer Normalization\n\nApplied after each sub-layer to:\n- **Stabilize Training**: Reduces internal covariate shift\n- **Accelerate Convergence**: Enables higher learning rates\n- **Independence**: Normalizes across features, not batch dimension",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, \n",
    "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Decoder Layer\n\nThe **Decoder Layer** extends the encoder architecture with an additional attention mechanism for **encoder-decoder attention**, enabling the model to condition generation on the input sequence.\n\n### Three-Stage Architecture\n\n1. **Masked Self-Attention**: Prevents positions from attending to future positions\n2. **Encoder-Decoder Attention**: Attends to the encoder output\n3. **Feed-Forward Network**: Position-wise transformations\n\nEach stage includes residual connections and layer normalization.\n\n### Masked Self-Attention\n\nUses a **look-ahead mask** to prevent the model from seeing future tokens during training:\n- **Causality**: Ensures autoregressive property\n- **Training Efficiency**: Allows parallel processing during training\n- **Inference Consistency**: Training matches inference behavior\n\n### Encoder-Decoder Attention (Cross-Attention)\n\nThe crucial mechanism for sequence-to-sequence tasks:\n- **Queries**: Come from the decoder (what we're generating)\n- **Keys & Values**: Come from the encoder (input context)\n- **Information Flow**: Allows decoder to access input information\n\nThis creates a differentiable pathway for information to flow from input to output, enabling learned translations, summarizations, and other sequence transformations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "# y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
    "# out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
    "# out.shape, attn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Complete Encoder\n\nThe **Encoder** stacks multiple encoder layers to build hierarchical representations of the input sequence.\n\n### Architecture Components\n\n1. **Token Embedding**: Converts discrete tokens to continuous vectors\n2. **Positional Encoding**: Adds positional information\n3. **Embedding Scaling**: Multiplies embeddings by $\\sqrt{d_{model}}$ for proper scaling\n4. **Stacked Layers**: Multiple encoder layers for deep representation learning\n5. **Dropout**: Regularization to prevent overfitting\n\n### Information Flow\n\nEach encoder layer refines the representation:\n- **Layer 1**: Basic token relationships and local patterns\n- **Layer 2**: More complex interactions and phrase-level understanding  \n- **Layer N**: Abstract, high-level representations\n\n### Differentiable Processing\n\nThe encoder creates a **differentiable transformation** from discrete token sequences to rich contextual representations, with gradients flowing through:\n- Attention patterns (learned token relationships)\n- Positional encodings (spatial understanding)\n- Feed-forward networks (non-linear transformations)\n- Residual connections (identity preservation)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Complete Decoder\n\nThe **Decoder** generates output sequences autoregressively while attending to the encoder's representation of the input.\n\n### Architecture Components\n\n1. **Target Embedding**: Embeds the target sequence tokens\n2. **Positional Encoding**: Adds positional information for generation\n3. **Stacked Decoder Layers**: Multiple layers with self-attention and cross-attention\n4. **Attention Weight Collection**: Tracks attention patterns for interpretability\n\n### Autoregressive Generation\n\nThe decoder generates sequences token by token:\n- **Training**: Processes entire target sequence with masking\n- **Inference**: Generates one token at a time, using previous outputs as input\n- **Causality**: Look-ahead masking ensures proper autoregressive behavior\n\n### Attention Pattern Storage\n\nThe decoder collects attention weights from each layer:\n- **Self-attention patterns**: How the decoder attends to previous tokens\n- **Cross-attention patterns**: How the decoder attends to encoder outputs\n- **Interpretability**: These patterns reveal learned alignments and dependencies\n\nThis enables analysis of what the model has learned and debugging of generation behavior.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
    "# sample_ffn(tf.random.uniform((64, 50, 512))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Complete Transformer Model\n\nThe **Transformer** combines the encoder and decoder into a complete sequence-to-sequence model.\n\n### Architecture Overview\n\n1. **Encoder**: Processes input sequence into contextual representations\n2. **Decoder**: Generates output sequence conditioned on encoder representations\n3. **Final Linear Layer**: Projects decoder output to vocabulary logits\n4. **End-to-End Training**: Entire model trained with next-token prediction\n\n### Information Flow\n\n```\nInput Tokens → Encoder → Context Representations\n                            ↓\nTarget Tokens → Decoder → Output Representations → Linear → Logits\n```\n\n### Differentiable Sequence-to-Sequence Learning\n\nThe Transformer creates a fully differentiable pathway from input sequences to output sequences:\n\n- **Encoder-Decoder Attention**: Differentiable alignment between input and output\n- **Self-Attention**: Differentiable modeling of sequence dependencies  \n- **Autoregressive Training**: Teacher forcing enables parallel training\n- **End-to-End Gradients**: Loss gradients flow through the entire architecture\n\nThis makes the Transformer an ideal architecture for differentiable programming applications where we need to learn complex input-output mappings while maintaining interpretability through attention mechanisms.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Model Configuration\n\nHere we define the hyperparameters for a small Transformer model suitable for experimentation and learning:\n\n### Architecture Parameters\n\n- **num_layers = 4**: Number of encoder/decoder layers (relatively shallow for fast training)\n- **d_model = 128**: Model dimension (smaller than typical production models)\n- **dff = 512**: Feed-forward network dimension (4x model dimension)\n- **num_heads = 8**: Number of attention heads\n\n### Vocabulary Configuration\n\n- **input_vocab_size = 10**: Small vocabulary for demonstration\n- **target_vocab_size = 2**: Binary output vocabulary (suitable for classification tasks)\n\n### Training Parameters\n\n- **dropout_rate = 0.1**: Standard dropout rate for regularization\n\nThis configuration creates a compact Transformer suitable for educational purposes while maintaining all the key architectural components of larger models.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Summary: Transformers in Differentiable Programming\n\nThe Transformer architecture represents a pinnacle of differentiable programming design, demonstrating several key principles:\n\n### Architectural Innovations\n\n1. **Pure Attention**: Eliminates recurrence while maintaining sequence modeling capability\n2. **Parallel Processing**: All positions processed simultaneously for efficiency\n3. **Hierarchical Representations**: Stacked layers build increasingly abstract representations\n4. **Residual Connections**: Enable deep architectures through improved gradient flow\n\n### Differentiable Programming Principles\n\n- **End-to-End Learning**: Entire model optimized jointly with gradient descent\n- **Modular Design**: Components can be mixed, matched, and reused\n- **Interpretable Attention**: Learned attention patterns provide insight into model behavior\n- **Scalable Architecture**: Principles scale from small experiments to large language models\n\n### Applications Beyond NLP\n\nThe Transformer's differentiable design has inspired applications across domains:\n- **Computer Vision**: Vision Transformers (ViTs) for image classification\n- **Scientific Computing**: Protein structure prediction (AlphaFold)\n- **Reinforcement Learning**: Decision Transformers for sequential decision making\n- **Multimodal Learning**: Cross-modal attention for vision-language tasks\n\n### Connection to Other Notebooks\n\nThis Transformer implementation connects to other differentiable programming concepts explored in this handbook:\n- **Custom Gradients**: Attention mechanisms can benefit from custom gradient implementations\n- **Differentiable Data Structures**: Attention creates differentiable memory access patterns\n- **Boolean Logic**: Self-attention can learn to implement logical operations\n- **Algorithmic Learning**: Transformers can learn to execute algorithms through attention patterns\n\nThe Transformer demonstrates how sophisticated computational patterns can emerge from simple, differentiable building blocks when combined with appropriate training objectives and architectural innovations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
    "\n",
    "# sample_encoder_layer_output = sample_encoder_layer(\n",
    "#     tf.random.uniform((64, 43, 512)), False, None)\n",
    "\n",
    "# sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "        \n",
    "#         tf.print(out1.shape)\n",
    "#         tf.print(enc_output.shape)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(\n",
    "            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
    "\n",
    "# sample_decoder_layer_output, block1, block2 = sample_decoder_layer(\n",
    "#     tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
    "#     False, None, None)\n",
    "\n",
    "# # (batch_size, target_seq_len, d_model)\n",
    "# print(sample_decoder_layer_output.shape, block1.shape, block2.shape)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
    "               maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.d_model)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
    "#                          dff=2048, input_vocab_size=8500,\n",
    "#                          maximum_position_encoding=10000)\n",
    "# temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "# sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
    "\n",
    "# print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
    "        maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model, num_heads, dff, rate) \n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
    "                look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
    "#                          dff=2048, target_vocab_size=8000,\n",
    "#                          maximum_position_encoding=5000)\n",
    "# temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "# output, attn = sample_decoder(temp_input, \n",
    "#                               enc_output=sample_encoder_output, \n",
    "#                               training=False,\n",
    "#                               look_ahead_mask=None, \n",
    "#                               padding_mask=None)\n",
    "\n",
    "# output.shape, attn['decoder_layer2_block2'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
    "            target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(\n",
    "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_transformer = Transformer(\n",
    "#     num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
    "#     input_vocab_size=8500, target_vocab_size=8000, \n",
    "#     pe_input=10000, pe_target=6000)\n",
    "\n",
    "# temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
    "# temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
    "\n",
    "# fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
    "#                                enc_padding_mask=None, \n",
    "#                                look_ahead_mask=None,\n",
    "#                                dec_padding_mask=None)\n",
    "\n",
    "# fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "\n",
    "input_vocab_size = 10\n",
    "target_vocab_size = 2\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
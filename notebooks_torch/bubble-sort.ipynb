{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable Bubble Sort\n",
    "\n",
    "Differentiable implementation of bubble sort with configurable (learnable) comparator function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu126\n",
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Check PyTorch installation and device availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable Swap Function\n",
    "\n",
    "The core of differentiable sorting is implementing a continuous approximation of the swap operation. We use linear interpolation to create a differentiable swap function.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "Given two elements `a` and `b`, and a swap parameter `t`:\n",
    "\n",
    "$$\n",
    "\\text{new}_a = a \\cdot t + b \\cdot (1 - t)\n",
    "$$\n",
    "$$\n",
    "\\text{new}_b = b \\cdot t + a \\cdot (1 - t)  \n",
    "$$\n",
    "\n",
    "**Key Properties:**\n",
    "- When `t = 0`: Elements are fully swapped (`a ↔ b`)\n",
    "- When `t = 1`: Elements remain unchanged  \n",
    "- Values between 0 and 1 create partial swaps, enabling gradient flow\n",
    "\n",
    "**Alternative Approaches:**\n",
    "This notebook uses linear interpolation for simplicity and interpretability. Other differentiable sorting strategies include:\n",
    "- [Softmax-based approximations](https://github.com/johnhw/differentiable_sorting) \n",
    "- [Optimal transport methods](https://arxiv.org/pdf/1905.11885.pdf)\n",
    "- [Higher-dimensional projections](https://arxiv.org/pdf/2002.08871.pdf)\n",
    "\n",
    "Each approach has different trade-offs in terms of approximation quality, computational efficiency, and gradient properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap(x, i, j, t=None):\n",
    "    \"\"\"\n",
    "    Differentiable swap function using linear interpolation.\n",
    "\n",
    "    Args:\n",
    "        x: torch.Tensor - Input tensor of shape [sequence_length, feature_size]\n",
    "        i: int - First index to swap\n",
    "        j: int - Second index to swap\n",
    "        t: torch.Tensor - Swap parameter (scalar or tensor)\n",
    "                         If None, defaults to full swap (t=0)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor - Tensor with elements at positions i and j interpolated\n",
    "    \"\"\"\n",
    "    if t is None:\n",
    "        t = torch.tensor(0.0)\n",
    "\n",
    "    # Ensure t is a scalar for consistent behavior\n",
    "    if isinstance(t, torch.Tensor) and t.numel() > 1:\n",
    "        t = t.mean()  # Use mean if t is a tensor\n",
    "\n",
    "    # Clone to avoid in-place operations\n",
    "    result = x.clone()\n",
    "\n",
    "    # Linear interpolation swap\n",
    "    # t=0: full swap, t=1: no swap\n",
    "    result[i] = t * x[i] + (1 - t) * x[j]\n",
    "    result[j] = t * x[j] + (1 - t) * x[i]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swapped tensor:\n",
      "tensor([[1., 1., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 1., 0., 1.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0.]], grad_fn=<CopySlices>)\n",
      "\n",
      "Gradient w.r.t. input x:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n",
      "Gradient w.r.t. swap parameter t:\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Test the swap function with gradients\n",
    "x = torch.tensor(\n",
    "    [[1, 1, 0, 0], [1, 1, 0, 1], [1, 0, 0, 0], [1, 0, 1, 0], [1, 1, 1, 0]],\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True,\n",
    ")\n",
    "\n",
    "t = torch.zeros_like(x, requires_grad=True)\n",
    "i, j = 1, 2\n",
    "\n",
    "# Forward pass\n",
    "z = swap(x, i, j, t)\n",
    "print(\"Swapped tensor:\")\n",
    "print(z)\n",
    "\n",
    "# Compute gradients\n",
    "loss = z.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradient w.r.t. input x:\")\n",
    "print(x.grad)\n",
    "print(\"\\nGradient w.r.t. swap parameter t:\")\n",
    "print(t.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "Swapped tensor:\n",
      "tensor([[2.],\n",
      "        [1.],\n",
      "        [3.]], grad_fn=<CopySlices>)\n",
      "Gradient w.r.t. x:\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n",
      "Gradient w.r.t. t:\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "# Simple test case with 1D vectors\n",
    "x = torch.tensor([[1.0], [2.0], [3.0]], dtype=torch.float32, requires_grad=True)\n",
    "t = torch.zeros_like(x, requires_grad=True)\n",
    "i, j = 0, 1\n",
    "\n",
    "z = swap(x, i, j, t)\n",
    "print(\"Original tensor:\")\n",
    "print(x.detach())\n",
    "print(\"Swapped tensor:\")\n",
    "print(z)\n",
    "\n",
    "# Test gradients\n",
    "loss = z.sum()\n",
    "loss.backward()\n",
    "print(\"Gradient w.r.t. x:\")\n",
    "print(x.grad)\n",
    "print(\"Gradient w.r.t. t:\")\n",
    "print(t.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable Bubble Sort Algorithm\n",
    "\n",
    "The bubble sort algorithm iteratively compares adjacent elements and swaps them if they're in the wrong order. In our differentiable version:\n",
    "\n",
    "1. **Comparator Function**: Instead of hard comparisons (`a > b`), we use a learned function that outputs a continuous swap parameter `t ∈ [0,1]`\n",
    "2. **Soft Swapping**: Elements are interpolated rather than discretely swapped, maintaining differentiability\n",
    "3. **End-to-End Learning**: The entire sorting process remains differentiable, allowing gradient-based optimization\n",
    "\n",
    "**Key Innovation**: The swap parameter `t` replaces discrete conditional logic, enabling backpropagation through the sorting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_sort(x, cmp_fun):\n",
    "    \"\"\"\n",
    "    Differentiable bubble sort implementation.\n",
    "\n",
    "    Args:\n",
    "        x: torch.Tensor - Input tensor of shape [sequence_length, feature_size]\n",
    "        cmp_fun: callable - Comparator function that takes a tensor of shape\n",
    "                           [batch_size, 2, feature_size] and returns swap parameters\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor - Sorted tensor (approximately, depending on comparator quality)\n",
    "    \"\"\"\n",
    "    x_len = x.shape[0]\n",
    "\n",
    "    # Bubble sort algorithm with differentiable comparisons\n",
    "    for i in range(x_len):\n",
    "        for j in range(i + 1, x_len):\n",
    "            # Prepare comparison input: stack elements i and j\n",
    "            cmp_input = torch.stack([x[i], x[j]], dim=0).unsqueeze(\n",
    "                0\n",
    "            )  # [1, 2, feature_size]\n",
    "\n",
    "            # Get swap parameter from comparator\n",
    "            t = cmp_fun(cmp_input)  # Returns [batch_size] tensor\n",
    "\n",
    "            # Apply differentiable swap\n",
    "            x = swap(x, i, j, t)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Comparator Function\n",
    "\n",
    "This is a simple rule-based comparator for demonstration. It sorts based on the sum of elements in each vector (number of 1s in binary vectors).\n",
    "\n",
    "**Note**: The `torch.sign` operation is non-differentiable, making this comparator unsuitable for end-to-end learning. It serves only as a test to verify our sorting logic works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparator input shapes: torch.Size([1, 2, 4])\n",
      "Element sums: tensor([1., 4.])\n",
      "Comparator output (swap parameter): tensor([1.])\n",
      "Interpretation: t=1 means 'don't swap' (first element should come first)\n"
     ]
    }
   ],
   "source": [
    "def sample_comparator(x):\n",
    "    \"\"\"\n",
    "    Rule-based comparator that sorts by sum of elements.\n",
    "\n",
    "    Args:\n",
    "        x: torch.Tensor - Shape [batch_size, 2, feature_size]\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor - Swap parameters of shape [batch_size]\n",
    "    \"\"\"\n",
    "    # Sum elements for each vector\n",
    "    sums = x.sum(dim=-1)  # [batch_size, 2]\n",
    "    diff = sums[:, 0] - sums[:, 1]  # Compare first vs second element\n",
    "\n",
    "    # Convert to swap parameter: 0 = swap, 1 = don't swap\n",
    "    # If first element sum > second element sum, don't swap (t=1)\n",
    "    # If first element sum < second element sum, swap (t=0)\n",
    "    return 1 - (torch.sign(diff) + 1) / 2\n",
    "\n",
    "\n",
    "# Test the sample comparator\n",
    "x_test = torch.tensor(\n",
    "    [[1, 0, 0, 0], [1, 1, 1, 1]], dtype=torch.float32  # sum = 1  # sum = 4\n",
    ")\n",
    "\n",
    "cmp_input = x_test.unsqueeze(0)  # Add batch dimension\n",
    "cmp_result = sample_comparator(cmp_input)\n",
    "print(f\"Comparator input shapes: {cmp_input.shape}\")\n",
    "print(f\"Element sums: {x_test.sum(dim=-1)}\")\n",
    "print(f\"Comparator output (swap parameter): {cmp_result}\")\n",
    "print(\"Interpretation: t=1 means 'don't swap' (first element should come first)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([3., 1., 2.])\n",
      "Sorted tensor:\n",
      "tensor([1., 2., 3.])\n",
      "Gradient w.r.t. input:\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# Test bubble sort with simple 1D case\n",
    "x = torch.tensor([[3.0], [1.0], [2.0]], dtype=torch.float32, requires_grad=True)\n",
    "print(\"Original tensor:\")\n",
    "print(x.detach().flatten())\n",
    "\n",
    "sorted_x = bubble_sort(x, sample_comparator)\n",
    "print(\"Sorted tensor:\")\n",
    "print(sorted_x.detach().flatten())\n",
    "\n",
    "# Test gradient computation\n",
    "loss = sorted_x.sum()\n",
    "loss.backward()\n",
    "print(\"Gradient w.r.t. input:\")\n",
    "print(x.grad.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor (rows sorted by number of 1s):\n",
      "tensor([[1., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 1., 1.]])\n",
      "Number of 1s per row: tensor([2., 1., 3.])\n",
      "\n",
      "Sorted tensor:\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "Number of 1s per row: tensor([1., 2., 3.])\n",
      "\n",
      "Gradient w.r.t. input:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Test with binary vectors (sorting by number of 1s)\n",
    "x = torch.tensor(\n",
    "    [[1, 1, 0], [1, 0, 0], [1, 1, 1]],  # 2 ones  # 1 one  # 3 ones\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True,\n",
    ")\n",
    "\n",
    "print(\"Original tensor (rows sorted by number of 1s):\")\n",
    "print(x.detach())\n",
    "print(\"Number of 1s per row:\", x.sum(dim=1).detach())\n",
    "\n",
    "sorted_x = bubble_sort(x, sample_comparator)\n",
    "print(\"\\nSorted tensor:\")\n",
    "print(sorted_x.detach())\n",
    "print(\"Number of 1s per row:\", sorted_x.sum(dim=1).detach())\n",
    "\n",
    "# Compute gradients\n",
    "loss = sorted_x.sum()\n",
    "loss.backward()\n",
    "print(\"\\nGradient w.r.t. input:\")\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[1., 1., 0., 0.],\n",
      "        [1., 1., 0., 1.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 1., 1., 0.]])\n",
      "Number of 1s per row: tensor([2., 3., 1., 2., 3.])\n",
      "\n",
      "Sorted tensor (should be ordered by number of 1s):\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.5000, 0.5000, 0.0000],\n",
      "        [1.0000, 0.5000, 0.5000, 0.0000],\n",
      "        [1.0000, 1.0000, 0.5000, 0.5000],\n",
      "        [1.0000, 1.0000, 0.5000, 0.5000]])\n",
      "Number of 1s per row: tensor([1., 2., 2., 3., 3.])\n",
      "\n",
      "Gradient w.r.t. input:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Test with more complex binary vectors\n",
    "x = torch.tensor(\n",
    "    [\n",
    "        [1, 1, 0, 0],  # 2 ones\n",
    "        [1, 1, 0, 1],  # 3 ones\n",
    "        [1, 0, 0, 0],  # 1 one\n",
    "        [1, 0, 1, 0],  # 2 ones\n",
    "        [1, 1, 1, 0],  # 3 ones\n",
    "    ],\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True,\n",
    ")\n",
    "\n",
    "print(\"Original tensor:\")\n",
    "print(x.detach())\n",
    "print(\"Number of 1s per row:\", x.sum(dim=1).detach())\n",
    "\n",
    "sorted_x = bubble_sort(x, sample_comparator)\n",
    "print(\"\\nSorted tensor (should be ordered by number of 1s):\")\n",
    "print(sorted_x.detach())\n",
    "print(\"Number of 1s per row:\", sorted_x.sum(dim=1).detach())\n",
    "\n",
    "# Compute gradients\n",
    "loss = sorted_x.sum()\n",
    "loss.backward()\n",
    "print(\"\\nGradient w.r.t. input:\")\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sorted data (lower triangular matrix):\n",
      "Each row should have 1, 2, 3, ..., 10 ones respectively\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Generate test data: lower triangular matrix\n",
    "# Each row has an increasing number of 1s (from 1 to 10)\n",
    "def generate_data():\n",
    "    return np.tril(np.ones((10, 10), dtype=np.float32))\n",
    "\n",
    "\n",
    "actual_data = generate_data()\n",
    "print(\"Target sorted data (lower triangular matrix):\")\n",
    "print(\"Each row should have 1, 2, 3, ..., 10 ones respectively\")\n",
    "print(actual_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled data to be sorted:\n",
      "Number of 1s per row: [ 1.  6.  3.  7. 10.  9.  4.  2.  5.  8.]\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 0.]\n",
      " [1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create shuffled version for testing\n",
    "shuffled_data = generate_data()\n",
    "np.random.shuffle(shuffled_data)\n",
    "print(\"Shuffled data to be sorted:\")\n",
    "print(\"Number of 1s per row:\", shuffled_data.sum(axis=1))\n",
    "print(shuffled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted result:\n",
      "Number of 1s per row: tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "Verification - should be all zeros if sorting is perfect:\n",
      "Max absolute difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Test sorting with the rule-based comparator\n",
    "shuffled_tensor = torch.tensor(shuffled_data, dtype=torch.float32)\n",
    "sorted_result = bubble_sort(shuffled_tensor, sample_comparator)\n",
    "\n",
    "print(\"Sorted result:\")\n",
    "print(\"Number of 1s per row:\", sorted_result.sum(dim=1).detach())\n",
    "print(sorted_result.detach())\n",
    "\n",
    "print(\"\\nVerification - should be all zeros if sorting is perfect:\")\n",
    "difference = sorted_result.detach() - torch.tensor(actual_data)\n",
    "print(\"Max absolute difference:\", torch.abs(difference).max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learnable Neural Comparator\n",
    "\n",
    "The key innovation is replacing the rule-based comparator with a learnable neural network. This enables:\n",
    "\n",
    "1. **End-to-end Learning**: The sorting criterion can be learned from data rather than hand-coded\n",
    "2. **Complex Patterns**: Neural networks can learn sophisticated comparison functions beyond simple rules\n",
    "3. **Gradient Flow**: Unlike rule-based comparators with discrete operations, neural comparators maintain differentiability\n",
    "\n",
    "**Architecture**: A simple MLP that takes two concatenated feature vectors and outputs a swap probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComparatorBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network comparator for differentiable sorting.\n",
    "\n",
    "    Takes two feature vectors and outputs a swap parameter t ∈ [0,1].\n",
    "    - t ≈ 0: Swap the elements (first element should come after second)\n",
    "    - t ≈ 1: Don't swap (first element should come before second)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_size, hidden_size=10):\n",
    "        super(ComparatorBlock, self).__init__()\n",
    "\n",
    "        # Input size is 2 * feature_size (two concatenated vectors)\n",
    "        input_size = 2 * feature_size\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid(),  # Output in [0,1] range\n",
    "        )\n",
    "\n",
    "        # Initialize weights using He initialization for ReLU networks\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.kaiming_normal_(layer.weight, nonlinearity=\"relu\")\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: torch.Tensor - Shape [batch_size, 2, feature_size]\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor - Swap parameters of shape [batch_size]\n",
    "        \"\"\"\n",
    "        batch_size, _, feature_size = x.shape\n",
    "\n",
    "        # Flatten the two vectors and concatenate them\n",
    "        x_flat = x.reshape(batch_size, -1)  # [batch_size, 2 * feature_size]\n",
    "\n",
    "        # Pass through network\n",
    "        output = self.network(x_flat).squeeze(-1)  # [batch_size]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural comparator output shape: torch.Size([1])\n",
      "Output value (random initialization): 0.5000\n",
      "\n",
      "Initial loss (before training): 0.1936\n",
      "This should be high since the comparator is randomly initialized\n"
     ]
    }
   ],
   "source": [
    "# Test the neural comparator before training\n",
    "feature_size = shuffled_data.shape[-1]\n",
    "learned_comparator = ComparatorBlock(feature_size)\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.zeros((1, 2, feature_size))\n",
    "test_output = learned_comparator(test_input)\n",
    "print(f\"Neural comparator output shape: {test_output.shape}\")\n",
    "print(f\"Output value (random initialization): {test_output.item():.4f}\")\n",
    "\n",
    "# Test sorting with untrained comparator\n",
    "shuffled_tensor = torch.tensor(shuffled_data, dtype=torch.float32)\n",
    "untrained_result = bubble_sort(shuffled_tensor, learned_comparator)\n",
    "\n",
    "# Compute initial loss (L2 distance to target)\n",
    "target_tensor = torch.tensor(actual_data, dtype=torch.float32)\n",
    "initial_loss = F.mse_loss(untrained_result, target_tensor)\n",
    "print(f\"\\nInitial loss (before training): {initial_loss.item():.4f}\")\n",
    "print(\"This should be high since the comparator is randomly initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.1628\n",
      "Input gradient norm: 0.0326\n",
      "Number of comparator parameters: 331\n",
      "Comparator gradient norm: 0.0984\n",
      "✓ Gradients are flowing through the entire pipeline!\n"
     ]
    }
   ],
   "source": [
    "# Test gradient computation through the learned comparator\n",
    "learned_comparator = ComparatorBlock(feature_size)\n",
    "shuffled_tensor = torch.tensor(shuffled_data, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "sorted_result = bubble_sort(shuffled_tensor, learned_comparator)\n",
    "target_tensor = torch.tensor(actual_data, dtype=torch.float32)\n",
    "loss = F.mse_loss(sorted_result, target_tensor)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "print(f\"Input gradient norm: {shuffled_tensor.grad.norm().item():.4f}\")\n",
    "print(\n",
    "    f\"Number of comparator parameters: {sum(p.numel() for p in learned_comparator.parameters())}\"\n",
    ")\n",
    "\n",
    "# Check if gradients flow to the comparator\n",
    "total_grad_norm = 0\n",
    "for param in learned_comparator.parameters():\n",
    "    if param.grad is not None:\n",
    "        total_grad_norm += param.grad.norm().item() ** 2\n",
    "total_grad_norm = total_grad_norm**0.5\n",
    "\n",
    "print(f\"Comparator gradient norm: {total_grad_norm:.4f}\")\n",
    "print(\n",
    "    \"✓ Gradients are flowing through the entire pipeline!\"\n",
    "    if total_grad_norm > 0\n",
    "    else \"✗ No gradients in comparator\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Training\n",
    "\n",
    "Now we train the neural comparator to learn the sorting criterion. The network learns to output appropriate swap parameters by minimizing the reconstruction loss between the sorted output and the target sorted data.\n",
    "\n",
    "**Training Process:**\n",
    "1. **Forward Pass**: Input data → Neural Comparator → Bubble Sort → Output\n",
    "2. **Loss Computation**: MSE between sorted output and target  \n",
    "3. **Backward Pass**: Gradients flow back through the entire pipeline\n",
    "4. **Parameter Update**: Adam optimizer updates the comparator weights\n",
    "\n",
    "**Key Insight**: The network implicitly learns to count ones in binary vectors to determine the correct sorting order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the neural comparator...\n",
      "Epoch\tLoss\n",
      "--------------------\n",
      "0\t0.137729\n",
      "100\t0.088227\n",
      "200\t0.062058\n",
      "300\t0.044952\n",
      "400\t0.032504\n",
      "500\t0.023212\n",
      "600\t0.015396\n",
      "700\t0.007712\n",
      "800\t0.004370\n",
      "900\t0.002778\n",
      "Final loss: 0.001930\n"
     ]
    }
   ],
   "source": [
    "# Training setup\n",
    "learned_comparator = ComparatorBlock(feature_size)\n",
    "optimizer = torch.optim.Adam(learned_comparator.parameters(), lr=3e-4)\n",
    "\n",
    "# Training data\n",
    "shuffled_tensor = torch.tensor(shuffled_data, dtype=torch.float32, requires_grad=True)\n",
    "target_tensor = torch.tensor(actual_data, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def training_step():\n",
    "    \"\"\"Single training step\"\"\"\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass through sorting algorithm\n",
    "    sorted_result = bubble_sort(shuffled_tensor, learned_comparator)\n",
    "\n",
    "    # Compute reconstruction loss\n",
    "    loss = F.mse_loss(sorted_result, target_tensor)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "print(\"Training the neural comparator...\")\n",
    "print(\"Epoch\\tLoss\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    loss = training_step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"{epoch}\\t{loss:.6f}\")\n",
    "\n",
    "print(f\"Final loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final sorted result (after training):\n",
      "Number of 1s per row: tensor([1.0675, 2.0908, 3.0170, 3.9428, 5.0283, 6.0331, 7.0663, 7.9561, 8.8992,\n",
      "        9.8989])\n",
      "tensor([[1.0000e+00, 5.1918e-02, 1.2592e-02, 2.6614e-03, 2.2052e-04, 6.0783e-05,\n",
      "         4.6084e-06, 1.2963e-06, 3.4263e-07, 1.1421e-07],\n",
      "        [1.0000e+00, 9.6451e-01, 1.1318e-01, 1.1205e-02, 1.1693e-03, 7.0482e-04,\n",
      "         6.6414e-06, 3.1076e-06, 2.4658e-07, 4.1117e-08],\n",
      "        [1.0000e+00, 9.8763e-01, 8.8469e-01, 1.2927e-01, 9.6228e-03, 5.6742e-03,\n",
      "         6.6642e-05, 1.2689e-05, 2.3181e-06, 3.1993e-07],\n",
      "        [1.0000e+00, 9.9651e-01, 9.9118e-01, 8.7525e-01, 6.1498e-02, 1.6994e-02,\n",
      "         1.2856e-03, 6.8157e-05, 2.3096e-05, 1.8056e-06],\n",
      "        [1.0000e+00, 9.9965e-01, 9.9870e-01, 9.9044e-01, 9.5370e-01, 7.4372e-02,\n",
      "         9.7898e-03, 1.4965e-03, 1.7134e-04, 1.1284e-05],\n",
      "        [1.0000e+00, 9.9982e-01, 9.9971e-01, 9.9239e-01, 9.7648e-01, 9.0892e-01,\n",
      "         1.3861e-01, 1.3740e-02, 3.2069e-03, 1.8409e-04],\n",
      "        [1.0000e+00, 9.9998e-01, 9.9997e-01, 9.9904e-01, 9.9796e-01, 9.9584e-01,\n",
      "         8.7994e-01, 1.4752e-01, 4.4050e-02, 2.0482e-03],\n",
      "        [1.0000e+00, 9.9999e-01, 9.9999e-01, 9.9980e-01, 9.9948e-01, 9.9772e-01,\n",
      "         9.7804e-01, 8.9465e-01, 8.0420e-02, 5.9833e-03],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9994e-01, 9.9989e-01, 9.9971e-01,\n",
      "         9.9282e-01, 9.4685e-01, 8.7774e-01, 8.2293e-02],\n",
      "        [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 9.9999e-01, 9.9999e-01,\n",
      "         9.9945e-01, 9.9566e-01, 9.9438e-01, 9.0948e-01]])\n",
      "\n",
      "Rounded result:\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "Target:\n",
      "Number of 1s per row: tensor([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "\n",
      "Maximum absolute error: 0.0\n",
      "Perfect sorting achieved!\n",
      "✓ The neural network successfully learned to sort by counting 1s!\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained model\n",
    "with torch.no_grad():\n",
    "    # Get final sorted result\n",
    "    final_sorted = bubble_sort(shuffled_tensor, learned_comparator)\n",
    "    final_rounded = torch.round(final_sorted)\n",
    "\n",
    "    print(\"Final sorted result (after training):\")\n",
    "    print(\"Number of 1s per row:\", final_sorted.sum(dim=1))\n",
    "    print(final_sorted)\n",
    "\n",
    "    print(\"\\nRounded result:\")\n",
    "    print(final_rounded)\n",
    "\n",
    "    print(\"\\nTarget:\")\n",
    "    print(\"Number of 1s per row:\", target_tensor.sum(dim=1))\n",
    "    print(target_tensor)\n",
    "\n",
    "    # Check if sorting is perfect\n",
    "    difference = final_rounded - target_tensor\n",
    "    max_error = torch.abs(difference).max().item()\n",
    "\n",
    "    print(f\"\\nMaximum absolute error: {max_error}\")\n",
    "    print(\n",
    "        \"Perfect sorting achieved!\"\n",
    "        if max_error == 0\n",
    "        else f\"Some errors remain (max: {max_error})\"\n",
    "    )\n",
    "\n",
    "    if max_error == 0:\n",
    "        print(\"✓ The neural network successfully learned to sort by counting 1s!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the learned comparator on specific pairs:\n",
      "--------------------------------------------------\n",
      "Pair 1: sum=1 vs sum=2\n",
      "  Should swap: No\n",
      "  Comparator output: 0.963\n",
      "  Interpretation: No swap\n",
      "\n",
      "Pair 2: sum=3 vs sum=1\n",
      "  Should swap: Yes\n",
      "  Comparator output: 0.153\n",
      "  Interpretation: Swap\n",
      "\n",
      "Pair 3: sum=2 vs sum=2\n",
      "  Should swap: Either\n",
      "  Comparator output: 0.807\n",
      "  Interpretation: No swap\n",
      "\n",
      "The comparator has learned to compare based on the sum of elements!\n"
     ]
    }
   ],
   "source": [
    "# Additional analysis: Test the learned comparator directly\n",
    "print(\"Testing the learned comparator on specific pairs:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Create test pairs with the same feature size as training data (10 dimensions)\n",
    "test_pairs = [\n",
    "    # 1 vs 2 ones - should swap (t≈0)\n",
    "    ([1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    # 3 vs 1 ones - should not swap (t≈1)\n",
    "    ([1, 1, 1, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "    # 2 vs 2 ones - indifferent (t≈0.5)\n",
    "    ([1, 1, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (vec1, vec2) in enumerate(test_pairs):\n",
    "        # Prepare input with correct feature size\n",
    "        pair_input = torch.tensor([[vec1, vec2]], dtype=torch.float32)\n",
    "        swap_param = learned_comparator(pair_input).item()\n",
    "\n",
    "        sum1, sum2 = sum(vec1), sum(vec2)\n",
    "        should_swap = \"Yes\" if sum1 > sum2 else (\"No\" if sum1 < sum2 else \"Either\")\n",
    "\n",
    "        print(f\"Pair {i+1}: sum={sum1} vs sum={sum2}\")\n",
    "        print(f\"  Should swap: {should_swap}\")\n",
    "        print(f\"  Comparator output: {swap_param:.3f}\")\n",
    "        print(f\"  Interpretation: {'Swap' if swap_param < 0.5 else 'No swap'}\")\n",
    "        print()\n",
    "\n",
    "print(\"The comparator has learned to compare based on the sum of elements!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "crhg27i167c",
   "metadata": {},
   "source": [
    "# Differentiable Pathfinding with Dense Matrix Operations\n",
    "\n",
    "This notebook demonstrates how to solve pathfinding problems using differentiable programming. Instead of traditional graph algorithms, we use gradient descent to learn the optimal sequence of moves through a graph.\n",
    "\n",
    "## Key Concepts\n",
    "- **Differentiable discrete optimization**: Making discrete pathfinding continuous and learnable\n",
    "- **Matrix-based transitions**: Using adjacency matrices to constrain valid moves\n",
    "- **Parameter learning**: Learning step parameters that encode movement decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0f0790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u1i5j9e07bt",
   "metadata": {},
   "source": [
    "## Graph Definition\n",
    "\n",
    "Define the graph structure using an adjacency matrix. This 5-node graph is based on CLRS page 590 with edge 2-3 removed.\n",
    "\n",
    "**Graph structure:**\n",
    "- Nodes: 0, 1, 2, 3, 4  \n",
    "- Edges: 0-1, 0-4, 1-3, 1-4, 2-3, 3-4\n",
    "\n",
    "The adjacency matrix constrains movement to only valid connections between nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94686b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 1.],\n",
       "        [1., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 1., 0., 1.],\n",
       "        [1., 1., 0., 1., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CLRS 3rd edition page 590 but I removed 2-3 edge\n",
    "adjacency_matrix = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 0, 0, 1],\n",
    "        [1, 0, 0, 1, 1],\n",
    "        [0, 0, 0, 1, 0],\n",
    "        [0, 1, 1, 0, 1],\n",
    "        [1, 1, 0, 1, 0],\n",
    "    ],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "adjacency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6hh3hre3tda",
   "metadata": {},
   "source": [
    "## Experiment 1: Learning with Non-Zero Initialization\n",
    "\n",
    "**Problem Setup:**\n",
    "- Start at node 0: `[1, 0, 0, 0, 0]`\n",
    "- Target node 2: `[0, 0, 1, 0, 0]`\n",
    "- Need to find path: 0 → 1 → 3 → 2\n",
    "\n",
    "**Algorithm:**\n",
    "1. **State representation**: One-hot vectors for current position\n",
    "2. **Learnable parameters**: Three step parameters (initialized to 0.2) that control movement decisions\n",
    "3. **Forward pass**: For each step, multiply current state by step parameters, then by adjacency matrix\n",
    "4. **Loss**: MSE between final state and target position\n",
    "\n",
    "**Key insight**: The adjacency matrix multiplication `adjacency_matrix @ (state * step_params)` ensures only valid graph moves are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe964906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found correct answer at step 0\n",
      "Epoch 0, Loss: 0.1944\n",
      "Epoch 100, Loss: 0.1207\n",
      "Epoch 200, Loss: 0.0000\n",
      "Epoch 300, Loss: 0.0000\n",
      "Epoch 400, Loss: 0.0000\n",
      "Epoch 500, Loss: 0.0000\n",
      "Epoch 600, Loss: 0.0000\n",
      "Epoch 700, Loss: 0.0000\n",
      "Epoch 800, Loss: 0.0000\n",
      "Epoch 900, Loss: 0.0000\n",
      "\n",
      "Final state (0): tensor([-8.9432e-08,  1.7890e-07,  1.0000e+00, -8.9432e-08,  1.7881e-07],\n",
      "       grad_fn=<MvBackward0>)\n",
      "Step 1 (0) probs: tensor([0.9725, 0.2000, 0.2000, 0.2000, 0.2000])\n",
      "Step 2 (1) probs: tensor([0.2000, 0.7019, 0.2000, 0.2000, 0.7019])\n",
      "Step 3 (3) probs: tensor([-7.3255e-01, -4.3828e-08,  2.0000e-01,  7.3256e-01, -8.7199e-08])\n"
     ]
    }
   ],
   "source": [
    "# Starting node (one-hot)\n",
    "start = torch.tensor([1.0, 0.0, 0.0, 0.0, 0.0])\n",
    "# Target node (one-hot)\n",
    "target = torch.tensor([0.0, 0.0, 1.0, 0.0, 0.0])\n",
    "# 3 transition probability vectors (learnable parameters)\n",
    "step1 = nn.Parameter(torch.tensor([0.2, 0.2, 0.2, 0.2, 0.2]))\n",
    "step2 = nn.Parameter(torch.tensor([0.2, 0.2, 0.2, 0.2, 0.2]))\n",
    "step3 = nn.Parameter(torch.tensor([0.2, 0.2, 0.2, 0.2, 0.2]))\n",
    "optimizer = optim.SGD([step1, step2, step3], lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "found_correct = False\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: apply transitions\n",
    "    state = start\n",
    "    for step_params in [step1, step2, step3]:\n",
    "        # Negative numbers are required for inhibitory backprop\n",
    "        # step_probs = torch.softmax(step_params, dim=0)  # Proper probabilities\n",
    "        state = adjacency_matrix @ (state * step_params)\n",
    "        # state = state / (state.sum() + 1e-8)  # Normalize to maintain probability mass\n",
    "    # Loss: MSE to target\n",
    "    loss = nn.MSELoss()(state, target)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Check if we found the correct answer\n",
    "    if not found_correct:\n",
    "        step1_choice = torch.argmax(step1.data).item()\n",
    "        step2_choice = torch.argmax(step2.data).item()\n",
    "        step3_choice = torch.argmax(step3.data).item()\n",
    "\n",
    "        if step1_choice == 0 and step2_choice == 1 and step3_choice == 3:\n",
    "            print(f\"Found correct answer at step {epoch}\")\n",
    "            found_correct = True\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal state ({torch.argmax(step1.data)}): {state}\")\n",
    "print(f\"Step 1 ({torch.argmax(step1.data)}) probs: {step1.data}\")\n",
    "print(f\"Step 2 ({torch.argmax(step2.data)}) probs: {step2.data}\")\n",
    "print(f\"Step 3 ({torch.argmax(step3.data)}) probs: {step3.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1qniu2zmd",
   "metadata": {},
   "source": [
    "## Experiment 2: Zero Initialization Problem\n",
    "\n",
    "**Hypothesis**: What happens if we initialize all parameters to zero?\n",
    "\n",
    "This experiment demonstrates the **dead neuron problem** in neural networks. When all parameters start at zero:\n",
    "- All gradients become zero\n",
    "- No learning occurs (parameters remain at zero)  \n",
    "- The algorithm gets stuck and cannot find the solution\n",
    "\n",
    "**Result**: Loss remains constant at 0.2000, showing the importance of proper parameter initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e9bfe46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2000\n",
      "Epoch 100, Loss: 0.2000\n",
      "Epoch 200, Loss: 0.2000\n",
      "Epoch 300, Loss: 0.2000\n",
      "Epoch 400, Loss: 0.2000\n",
      "Epoch 500, Loss: 0.2000\n",
      "Epoch 600, Loss: 0.2000\n",
      "Epoch 700, Loss: 0.2000\n",
      "Epoch 800, Loss: 0.2000\n",
      "Epoch 900, Loss: 0.2000\n",
      "\n",
      "Final state (0): tensor([0., 0., 0., 0., 0.], grad_fn=<MvBackward0>)\n",
      "Step 1 (0) probs: tensor([0., 0., 0., 0., 0.])\n",
      "Step 2 (0) probs: tensor([0., 0., 0., 0., 0.])\n",
      "Step 3 (0) probs: tensor([0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Starting node (one-hot)\n",
    "start = torch.tensor([1.0, 0.0, 0.0, 0.0, 0.0])\n",
    "# Target node (one-hot)\n",
    "target = torch.tensor([0.0, 0.0, 1.0, 0.0, 0.0])\n",
    "# 3 transition probability vectors (learnable parameters)\n",
    "step1 = nn.Parameter(torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0]))\n",
    "step2 = nn.Parameter(torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0]))\n",
    "step3 = nn.Parameter(torch.tensor([0.0, 0.0, 0.0, 0.0, 0.0]))\n",
    "optimizer = optim.SGD([step1, step2, step3], lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "found_correct = False\n",
    "for epoch in range(1000):\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: apply transitions\n",
    "    state = start\n",
    "    for step_params in [step1, step2, step3]:\n",
    "        # Negative numbers are required for inhibitory backprop\n",
    "        # step_probs = torch.softmax(step_params, dim=0)  # Proper probabilities\n",
    "        state = adjacency_matrix @ (state * step_params)\n",
    "        # state = state / (state.sum() + 1e-8)  # Normalize to maintain probability mass\n",
    "    # Loss: MSE to target\n",
    "    loss = nn.MSELoss()(state, target)\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Check if we found the correct answer\n",
    "    if not found_correct:\n",
    "        step1_choice = torch.argmax(step1.data).item()\n",
    "        step2_choice = torch.argmax(step2.data).item()\n",
    "        step3_choice = torch.argmax(step3.data).item()\n",
    "\n",
    "        if step1_choice == 0 and step2_choice == 1 and step3_choice == 3:\n",
    "            print(f\"Found correct answer at step {epoch}\")\n",
    "            found_correct = True\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nFinal state ({torch.argmax(step1.data)}): {state}\")\n",
    "print(f\"Step 1 ({torch.argmax(step1.data)}) probs: {step1.data}\")\n",
    "print(f\"Step 2 ({torch.argmax(step2.data)}) probs: {step2.data}\")\n",
    "print(f\"Step 3 ({torch.argmax(step3.data)}) probs: {step3.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jwoird6pfd",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Differentiable pathfinding**: Discrete optimization problems can be made continuous and solved with gradient descent\n",
    "2. **Adjacency constraints**: Matrix multiplication naturally enforces graph structure constraints  \n",
    "3. **Initialization matters**: Zero initialization creates dead neurons and prevents learning\n",
    "4. **Negative parameters**: The algorithm allows negative values for \"inhibitory\" effects\n",
    "5. **Matrix operations**: Dense matrix operations can encode complex discrete decision-making processes\n",
    "\n",
    "**Applications**: This approach can be extended to larger graphs, multiple agents, or dynamic pathfinding scenarios where traditional algorithms might be insufficient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiable Stacks\n",
    "\n",
    "There exist many implementations of differentiable stacks in the literature related to neural turing machines and similar. e.g. [Learning to Transduce with Unbounded Memory](http://papers.nips.cc/paper/5648-learning-to-transduce-with-unbounded-memory.pdf), [Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets](https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.pdf) etc.\n",
    "\n",
    "A differentiable stack maintains the LIFO (Last In, First Out) behavior of classical stacks while being fully differentiable for gradient-based optimization. This enables neural networks to learn algorithms that manipulate stack-like memory structures.\n",
    "\n",
    "**Key principles:**\n",
    "1. **Deterministic and lossless forward pass** - Operations preserve information exactly\n",
    "2. **Well-defined gradients** - All operations support backpropagation through PyTorch's automatic differentiation\n",
    "\n",
    "PyTorch's autograd seamlessly handles gradient flow through our stack operations, making the differentiable implementation nearly identical to classical stack operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differentiable Stack Module\n",
    "\n",
    "The stack uses two components:\n",
    "- **Buffer**: A fixed-size tensor that stores all stack elements \n",
    "- **Index**: A one-hot vector indicating the current top position + 1\n",
    "\n",
    "**Soft operations**: Instead of discrete indexing (which breaks differentiability), we use:\n",
    "- Soft assignment using weighted combinations\n",
    "- Superposition lookup using attention-like mechanisms  \n",
    "- One-hot vectors shifted with `torch.roll` to track position\n",
    "\n",
    "This PyTorch module encapsulates all stack operations while maintaining full differentiability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial stack buffer:\n",
      "Parameter containing:\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], requires_grad=True)\n",
      "Initial stack index: Parameter containing:\n",
      "tensor([1., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class DifferentiableStack(nn.Module):\n",
    "    \"\"\"A differentiable stack data structure implemented as a PyTorch module.\n",
    "\n",
    "    Note: This implementation uses in-place operations for demonstration purposes.\n",
    "    For gradient-based learning, use the functional operations in ListReverser.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, stack_shape, device=None):\n",
    "        \"\"\"Initialize the differentiable stack.\n",
    "\n",
    "        Args:\n",
    "            stack_shape: Shape of the stack buffer (max_size, element_dim, ...)\n",
    "            device: Device to place tensors on (defaults to CPU)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if device is None:\n",
    "            device = torch.device(\"cpu\")\n",
    "\n",
    "        self.stack_shape = stack_shape\n",
    "        self.device = device\n",
    "\n",
    "        # Initialize buffer and index as learnable parameters\n",
    "        buffer = torch.zeros(stack_shape, dtype=torch.float32, device=device)\n",
    "        index = F.one_hot(torch.tensor(0, device=device), stack_shape[0]).float()\n",
    "\n",
    "        self.register_parameter(\"buffer\", nn.Parameter(buffer))\n",
    "        self.register_parameter(\"index\", nn.Parameter(index))\n",
    "\n",
    "    def _soft_assign(self, buffer, index, element):\n",
    "        \"\"\"Soft assignment operation for differentiable indexing.\"\"\"\n",
    "        if buffer.dim() == 1:\n",
    "            return buffer + index * (element - buffer)\n",
    "        else:\n",
    "            # Expand index to match buffer dimensions\n",
    "            for _ in range(buffer.dim() - 1):\n",
    "                index = index.unsqueeze(-1)\n",
    "            return buffer + index * (element.unsqueeze(0) - buffer)\n",
    "\n",
    "    def _soft_lookup(self, buffer, index):\n",
    "        \"\"\"Soft lookup operation for differentiable indexing.\"\"\"\n",
    "        if buffer.dim() == 1:\n",
    "            return torch.sum(index * buffer)\n",
    "        else:\n",
    "            # Expand index to match buffer dimensions\n",
    "            for _ in range(buffer.dim() - 1):\n",
    "                index = index.unsqueeze(-1)\n",
    "            return torch.sum(index * buffer, dim=0)\n",
    "\n",
    "    def push(self, element):\n",
    "        \"\"\"Push an element onto the stack.\n",
    "\n",
    "        Note: Uses .data assignment which breaks gradients for optimization.\n",
    "        For learnable operations, use functional approach in ListReverser.\n",
    "        \"\"\"\n",
    "        # Update buffer at current index position\n",
    "        self.buffer.data = self._soft_assign(self.buffer, self.index, element)\n",
    "        # Shift index pointer forward\n",
    "        self.index.data = torch.roll(self.index, shifts=1, dims=0)\n",
    "\n",
    "    def pop(self):\n",
    "        \"\"\"Pop an element from the stack.\"\"\"\n",
    "        # Shift index pointer back\n",
    "        self.index.data = torch.roll(self.index, shifts=-1, dims=0)\n",
    "        # Get element at current position\n",
    "        element = self._soft_lookup(self.buffer, self.index)\n",
    "        return element\n",
    "\n",
    "    def peek(self):\n",
    "        \"\"\"Peek at the top element without removing it.\"\"\"\n",
    "        # Get index of top element\n",
    "        peek_index = torch.roll(self.index, shifts=-1, dims=0)\n",
    "        element = self._soft_lookup(self.buffer, peek_index)\n",
    "        return element\n",
    "\n",
    "\n",
    "# Create and test the stack\n",
    "stack = DifferentiableStack((3, 3))\n",
    "print(\"Initial stack buffer:\")\n",
    "print(stack.buffer)\n",
    "print(\"Initial stack index:\", stack.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stack from buffer:\n",
      "Buffer:\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n",
      "Index: Parameter containing:\n",
      "tensor([1., 0., 0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "class StackFromBuffer(nn.Module):\n",
    "    \"\"\"Create a differentiable stack from an existing buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer):\n",
    "        \"\"\"Initialize stack with pre-existing buffer.\n",
    "\n",
    "        Args:\n",
    "            buffer: Pre-existing tensor to use as stack buffer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Register buffer as parameter\n",
    "        self.register_parameter(\"buffer\", nn.Parameter(buffer.clone()))\n",
    "\n",
    "        # Initialize index pointing to first position\n",
    "        device = buffer.device\n",
    "        stack_shape = buffer.shape\n",
    "        index = F.one_hot(torch.tensor(0, device=device), stack_shape[0]).float()\n",
    "        self.register_parameter(\"index\", nn.Parameter(index))\n",
    "\n",
    "    def _soft_assign(self, buffer, index, element):\n",
    "        \"\"\"Soft assignment operation for differentiable indexing.\"\"\"\n",
    "        if buffer.dim() == 1:\n",
    "            return buffer + index * (element - buffer)\n",
    "        else:\n",
    "            for _ in range(buffer.dim() - 1):\n",
    "                index = index.unsqueeze(-1)\n",
    "            return buffer + index * (element.unsqueeze(0) - buffer)\n",
    "\n",
    "    def _soft_lookup(self, buffer, index):\n",
    "        \"\"\"Soft lookup operation for differentiable indexing.\"\"\"\n",
    "        if buffer.dim() == 1:\n",
    "            return torch.sum(index * buffer)\n",
    "        else:\n",
    "            for _ in range(buffer.dim() - 1):\n",
    "                index = index.unsqueeze(-1)\n",
    "            return torch.sum(index * buffer, dim=0)\n",
    "\n",
    "    def push(self, element):\n",
    "        \"\"\"Push an element onto the stack.\"\"\"\n",
    "        self.buffer.data = self._soft_assign(self.buffer, self.index, element)\n",
    "        self.index.data = torch.roll(self.index, shifts=1, dims=0)\n",
    "\n",
    "    def pop(self):\n",
    "        \"\"\"Pop an element from the stack.\"\"\"\n",
    "        self.index.data = torch.roll(self.index, shifts=-1, dims=0)\n",
    "        element = self._soft_lookup(self.buffer, self.index)\n",
    "        return element\n",
    "\n",
    "    def peek(self):\n",
    "        \"\"\"Peek at the top element without removing it.\"\"\"\n",
    "        peek_index = torch.roll(self.index, shifts=-1, dims=0)\n",
    "        element = self._soft_lookup(self.buffer, peek_index)\n",
    "        return element\n",
    "\n",
    "\n",
    "# Example: Create stack from existing buffer\n",
    "buffer = torch.ones((3, 3), dtype=torch.float32)\n",
    "stack_from_buffer = StackFromBuffer(buffer)\n",
    "print(\"Stack from buffer:\")\n",
    "print(\"Buffer:\")\n",
    "print(stack_from_buffer.buffer)\n",
    "print(\"Index:\", stack_from_buffer.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Soft Operations\n",
    "\n",
    "The key to differentiable stacks lies in **soft operations** that approximate discrete indexing while maintaining gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original buffer:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "Index (one-hot, points to row 1): tensor([0., 1., 0.])\n",
      "Element to assign: tensor([10., 11., 12.])\n",
      "\n",
      "After soft assignment:\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [10., 11., 12.],\n",
      "        [ 7.,  8.,  9.]])\n",
      "Soft lookup result: tensor([10., 11., 12.])\n"
     ]
    }
   ],
   "source": [
    "class SoftOperations(nn.Module):\n",
    "    \"\"\"Demonstrates the soft assignment and lookup operations used in differentiable stacks.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def soft_assign(self, buffer, index, element):\n",
    "        \"\"\"Soft assignment: buffer[i] = element (differentiable version)\n",
    "\n",
    "        Instead of discrete assignment, we use:\n",
    "        buffer = buffer + index * (element - buffer)\n",
    "\n",
    "        When index is one-hot [0,1,0], this updates only the selected position.\n",
    "        \"\"\"\n",
    "        if buffer.dim() == 1:\n",
    "            return buffer + index * (element - buffer)\n",
    "        else:\n",
    "            # Expand index to match buffer dimensions\n",
    "            for _ in range(buffer.dim() - 1):\n",
    "                index = index.unsqueeze(-1)\n",
    "            return buffer + index * (element.unsqueeze(0) - buffer)\n",
    "\n",
    "    def soft_lookup(self, buffer, index):\n",
    "        \"\"\"Soft lookup: element = buffer[i] (differentiable version)\n",
    "\n",
    "        Instead of discrete indexing, we use weighted sum:\n",
    "        element = sum(index * buffer)\n",
    "\n",
    "        When index is one-hot [0,1,0], this selects only one element.\n",
    "        \"\"\"\n",
    "        if buffer.dim() == 1:\n",
    "            return torch.sum(index * buffer)\n",
    "        else:\n",
    "            # Expand index to match buffer dimensions\n",
    "            for _ in range(buffer.dim() - 1):\n",
    "                index = index.unsqueeze(-1)\n",
    "            return torch.sum(index * buffer, dim=0)\n",
    "\n",
    "\n",
    "# Demonstrate soft operations\n",
    "soft_ops = SoftOperations()\n",
    "\n",
    "# Test buffer and index\n",
    "test_buffer = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]])\n",
    "test_index = torch.tensor([0.0, 1.0, 0.0])  # Points to second row\n",
    "test_element = torch.tensor([10.0, 11.0, 12.0])\n",
    "\n",
    "print(\"Original buffer:\")\n",
    "print(test_buffer)\n",
    "print(\"Index (one-hot, points to row 1):\", test_index)\n",
    "print(\"Element to assign:\", test_element)\n",
    "\n",
    "# Test soft assignment\n",
    "new_buffer = soft_ops.soft_assign(test_buffer, test_index, test_element)\n",
    "print(\"\\nAfter soft assignment:\")\n",
    "print(new_buffer)\n",
    "\n",
    "# Test soft lookup\n",
    "looked_up = soft_ops.soft_lookup(new_buffer, test_index)\n",
    "print(\"Soft lookup result:\", looked_up)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Operations with Gradient Tracking\n",
    "\n",
    "Let's demonstrate how stack operations maintain gradients through the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final buffer after pushing 3 elements:\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]], requires_grad=True)\n",
      "\n",
      "Gradients with respect to input elements:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class StackPushModule(nn.Module):\n",
    "    \"\"\"Demonstrates stack push operation with gradient tracking.\"\"\"\n",
    "\n",
    "    def __init__(self, stack_shape):\n",
    "        super().__init__()\n",
    "        # Create a fresh stack for each forward pass\n",
    "        self.stack_shape = stack_shape\n",
    "\n",
    "    def forward(self, elements):\n",
    "        \"\"\"Push multiple elements and return final buffer state.\"\"\"\n",
    "        # Create fresh stack for this forward pass\n",
    "        stack = DifferentiableStack(self.stack_shape)\n",
    "\n",
    "        for element in elements:\n",
    "            stack.push(element)\n",
    "        return stack.buffer\n",
    "\n",
    "\n",
    "# Create elements to push\n",
    "elements = torch.tensor(\n",
    "    [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0]],\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True,\n",
    ")\n",
    "\n",
    "# Create stack module\n",
    "stack_push_module = StackPushModule((3, 3))\n",
    "\n",
    "# Forward pass with gradient tracking\n",
    "final_buffer = stack_push_module(elements)\n",
    "\n",
    "print(\"Final buffer after pushing 3 elements:\")\n",
    "print(final_buffer)\n",
    "\n",
    "# Test gradients\n",
    "loss = final_buffer.sum()\n",
    "loss.backward()\n",
    "print(\"\\nGradients with respect to input elements:\")\n",
    "print(elements.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Pop Operation\n",
    "\n",
    "The pop operation retrieves the top element while updating the stack pointer. It demonstrates how we can perform lookups while maintaining differentiability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First popped element (should be [3,3,3]): tensor([3., 3., 3.], grad_fn=<SumBackward1>)\n",
      "Second popped element (should be [2,2,2]): tensor([2., 2., 2.], grad_fn=<SumBackward1>)\n",
      "Stack buffer after pops:\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]], requires_grad=True)\n",
      "Stack index after pops: Parameter containing:\n",
      "tensor([0., 1., 0.], requires_grad=True)\n",
      "\n",
      "Gradients with respect to buffer:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class StackPopModule(nn.Module):\n",
    "    \"\"\"Demonstrates stack pop operation with gradient tracking.\"\"\"\n",
    "\n",
    "    def __init__(self, initial_buffer):\n",
    "        super().__init__()\n",
    "        self.initial_buffer = initial_buffer\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Pop two elements and return them.\"\"\"\n",
    "        # Create fresh stack from initial buffer\n",
    "        stack = StackFromBuffer(self.initial_buffer)\n",
    "\n",
    "        element1 = stack.pop()\n",
    "        element2 = stack.pop()\n",
    "        return element1, element2, stack.buffer, stack.index\n",
    "\n",
    "\n",
    "# Create a buffer with initial values\n",
    "buffer = torch.tensor(\n",
    "    [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0]],\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True,\n",
    ")\n",
    "\n",
    "# Create stack module with pre-filled buffer\n",
    "stack_pop_module = StackPopModule(buffer)\n",
    "\n",
    "# Perform pop operations with gradient tracking\n",
    "element1, element2, final_buffer, final_index = stack_pop_module()\n",
    "\n",
    "print(\"First popped element (should be [3,3,3]):\", element1)\n",
    "print(\"Second popped element (should be [2,2,2]):\", element2)\n",
    "print(\"Stack buffer after pops:\")\n",
    "print(final_buffer)\n",
    "print(\"Stack index after pops:\", final_index)\n",
    "\n",
    "# Test gradients\n",
    "loss = element1.sum() + element2.sum()\n",
    "loss.backward()\n",
    "print(\"\\nGradients with respect to buffer:\")\n",
    "print(buffer.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stack Peek Operation\n",
    "\n",
    "The peek operation allows us to examine the top element without modifying the stack state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peeked element (should be [3,3,3]): tensor([3., 3., 3.], grad_fn=<SumBackward1>)\n",
      "Stack buffer (unchanged):\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.],\n",
      "        [3., 3., 3.]], requires_grad=True)\n",
      "Stack index (unchanged): Parameter containing:\n",
      "tensor([1., 0., 0.], requires_grad=True)\n",
      "\n",
      "Gradients with respect to buffer:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "class StackPeekModule(nn.Module):\n",
    "    \"\"\"Demonstrates stack peek operation with gradient tracking.\"\"\"\n",
    "\n",
    "    def __init__(self, initial_buffer):\n",
    "        super().__init__()\n",
    "        self.initial_buffer = initial_buffer\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Peek at the top element without modifying stack.\"\"\"\n",
    "        stack = StackFromBuffer(self.initial_buffer)\n",
    "        return stack.peek(), stack.buffer, stack.index\n",
    "\n",
    "\n",
    "# Create a buffer with initial values\n",
    "buffer = torch.tensor(\n",
    "    [[1.0, 1.0, 1.0], [2.0, 2.0, 2.0], [3.0, 3.0, 3.0]],\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True,\n",
    ")\n",
    "\n",
    "# Create stack module\n",
    "stack_peek_module = StackPeekModule(buffer)\n",
    "\n",
    "# Peek at top element\n",
    "peeked_element, final_buffer, final_index = stack_peek_module()\n",
    "\n",
    "print(\"Peeked element (should be [3,3,3]):\", peeked_element)\n",
    "print(\"Stack buffer (unchanged):\")\n",
    "print(final_buffer)\n",
    "print(\"Stack index (unchanged):\", final_index)\n",
    "\n",
    "# Test gradients\n",
    "loss = peeked_element.sum()\n",
    "loss.backward()\n",
    "print(\"\\nGradients with respect to buffer:\")\n",
    "print(buffer.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application: List Reversal with Dual Stacks\n",
    "\n",
    "A classic application demonstrating differentiable stacks: reversing a list using two stacks. This algorithm shows how complex operations can be learned end-to-end through backpropagation.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Push all elements from the input list into Stack 1\n",
    "2. Pop all elements from Stack 1 and push them into Stack 2  \n",
    "3. Stack 2's buffer contains the reversed list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [4., 4., 4., 4.]], requires_grad=True)\n",
      "\n",
      "Reversed array:\n",
      "tensor([[4., 4., 4., 4.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [1., 1., 1., 1.]], grad_fn=<AddBackward0>)\n",
      "\n",
      "Gradients with respect to input:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "class ListReverser(nn.Module):\n",
    "    \"\"\"Module that reverses a list using two differentiable stacks.\"\"\"\n",
    "\n",
    "    def __init__(self, max_length, element_dim):\n",
    "        super().__init__()\n",
    "        self.max_length = max_length\n",
    "        self.element_dim = element_dim\n",
    "\n",
    "    def _soft_assign(self, buffer, index, element):\n",
    "        \"\"\"Soft assignment operation for differentiable indexing.\"\"\"\n",
    "        if buffer.dim() == 1:\n",
    "            return buffer + index * (element - buffer)\n",
    "        else:\n",
    "            for _ in range(buffer.dim() - 1):\n",
    "                index = index.unsqueeze(-1)\n",
    "            return buffer + index * (element.unsqueeze(0) - buffer)\n",
    "\n",
    "    def _soft_lookup(self, buffer, index):\n",
    "        \"\"\"Soft lookup operation for differentiable indexing.\"\"\"\n",
    "        if buffer.dim() == 1:\n",
    "            return torch.sum(index * buffer)\n",
    "        else:\n",
    "            for _ in range(buffer.dim() - 1):\n",
    "                index = index.unsqueeze(-1)\n",
    "            return torch.sum(index * buffer, dim=0)\n",
    "\n",
    "    def _stack_push(self, buffer, index, element):\n",
    "        \"\"\"Functional stack push - returns new state.\"\"\"\n",
    "        new_buffer = self._soft_assign(buffer, index, element)\n",
    "        new_index = torch.roll(index, shifts=1, dims=0)\n",
    "        return new_buffer, new_index\n",
    "\n",
    "    def _stack_pop(self, buffer, index):\n",
    "        \"\"\"Functional stack pop - returns element and new state.\"\"\"\n",
    "        new_index = torch.roll(index, shifts=-1, dims=0)\n",
    "        element = self._soft_lookup(buffer, new_index)\n",
    "        return element, buffer, new_index\n",
    "\n",
    "    def forward(self, input_list):\n",
    "        \"\"\"Reverse a list using two stacks.\n",
    "\n",
    "        Args:\n",
    "            input_list: Tensor of shape (sequence_length, element_dim)\n",
    "\n",
    "        Returns:\n",
    "            Reversed list tensor\n",
    "        \"\"\"\n",
    "        seq_length = input_list.shape[0]\n",
    "        device = input_list.device\n",
    "\n",
    "        # Initialize two stacks\n",
    "        buffer1 = torch.zeros((seq_length, self.element_dim), device=device)\n",
    "        index1 = F.one_hot(torch.tensor(0, device=device), seq_length).float()\n",
    "\n",
    "        buffer2 = torch.zeros((seq_length, self.element_dim), device=device)\n",
    "        index2 = F.one_hot(torch.tensor(0, device=device), seq_length).float()\n",
    "\n",
    "        # Step 1: Push all elements into stack1\n",
    "        for i in range(seq_length):\n",
    "            buffer1, index1 = self._stack_push(buffer1, index1, input_list[i])\n",
    "\n",
    "        # Step 2: Transfer all elements from stack1 to stack2\n",
    "        for _ in range(seq_length):\n",
    "            element, buffer1, index1 = self._stack_pop(buffer1, index1)\n",
    "            buffer2, index2 = self._stack_push(buffer2, index2, element)\n",
    "\n",
    "        # Return the buffer of stack2 (contains reversed list)\n",
    "        return buffer2\n",
    "\n",
    "\n",
    "# Test the list reverser\n",
    "reverser = ListReverser(max_length=4, element_dim=4)\n",
    "\n",
    "# Create input array\n",
    "input_arr = torch.tensor(\n",
    "    [\n",
    "        [1.0, 1.0, 1.0, 1.0],\n",
    "        [2.0, 2.0, 2.0, 2.0],\n",
    "        [3.0, 3.0, 3.0, 3.0],\n",
    "        [4.0, 4.0, 4.0, 4.0],\n",
    "    ],\n",
    "    dtype=torch.float32,\n",
    "    requires_grad=True,\n",
    ")\n",
    "\n",
    "# Reverse the list\n",
    "reversed_arr = reverser(input_arr)\n",
    "\n",
    "print(\"Original array:\")\n",
    "print(input_arr)\n",
    "print(\"\\nReversed array:\")\n",
    "print(reversed_arr)\n",
    "\n",
    "# Test gradient flow\n",
    "loss = reversed_arr.sum()\n",
    "loss.backward()\n",
    "print(\"\\nGradients with respect to input:\")\n",
    "print(input_arr.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Through Backpropagation\n",
    "\n",
    "This demonstrates the power of differentiable stacks: we can train a neural network to learn the input that produces a desired output through gradient descent. The stack operations maintain full differentiability throughout the computation graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training to learn input that reverses to target...\n",
      "Initial input:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "Epoch   0, Loss: 3.500000\n",
      "Epoch  10, Loss: 1.278137\n",
      "Epoch  20, Loss: 0.343994\n",
      "Epoch  30, Loss: 0.054887\n",
      "Epoch  40, Loss: 0.016218\n",
      "Epoch  50, Loss: 0.008572\n",
      "Epoch  60, Loss: 0.006566\n",
      "Epoch  70, Loss: 0.001941\n",
      "Epoch  80, Loss: 0.000025\n",
      "Epoch  90, Loss: 0.000215\n",
      "\n",
      "Final learned input:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [4., 4., 4., 4.]], grad_fn=<RoundBackward0>)\n",
      "\n",
      "Target reversed output:\n",
      "tensor([[4., 4., 4., 4.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n",
      "Actual reversed output:\n",
      "tensor([[4., 4., 4., 4.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [1., 1., 1., 1.]], grad_fn=<RoundBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class LearnableListReverser(nn.Module):\n",
    "    \"\"\"Training module that learns to reverse lists through gradient descent.\"\"\"\n",
    "\n",
    "    def __init__(self, max_length, element_dim):\n",
    "        super().__init__()\n",
    "        self.reverser = ListReverser(max_length, element_dim)\n",
    "\n",
    "    def forward(self, input_list):\n",
    "        return self.reverser(input_list)\n",
    "\n",
    "\n",
    "# Set up the learning experiment\n",
    "reverser_model = LearnableListReverser(max_length=4, element_dim=4)\n",
    "\n",
    "# Learnable input (starts as all ones)\n",
    "learnable_input = nn.Parameter(torch.ones((4, 4), dtype=torch.float32))\n",
    "\n",
    "# Target: what we want the reversed output to be\n",
    "target_reversed = torch.tensor(\n",
    "    [\n",
    "        [4.0, 4.0, 4.0, 4.0],\n",
    "        [3.0, 3.0, 3.0, 3.0],\n",
    "        [2.0, 2.0, 2.0, 2.0],\n",
    "        [1.0, 1.0, 1.0, 1.0],\n",
    "    ],\n",
    "    dtype=torch.float32,\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam([learnable_input], lr=0.1)\n",
    "\n",
    "# Training loop\n",
    "print(\"Training to learn input that reverses to target...\")\n",
    "print(\"Initial input:\")\n",
    "print(learnable_input.data)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass: reverse the learnable input\n",
    "    reversed_output = reverser_model(learnable_input)\n",
    "\n",
    "    # Loss: how different is our output from the target?\n",
    "    loss = F.mse_loss(reversed_output, target_reversed)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:3d}, Loss: {loss.item():.6f}\")\n",
    "\n",
    "print(\"\\nFinal learned input:\")\n",
    "print(torch.round(learnable_input))\n",
    "print(\"\\nTarget reversed output:\")\n",
    "print(target_reversed)\n",
    "print(\"\\nActual reversed output:\")\n",
    "print(torch.round(reverser_model(learnable_input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook demonstrated differentiable stacks implemented as PyTorch modules:\n",
    "\n",
    "1. **Core Concepts**: Differentiable stacks maintain LIFO behavior while preserving gradients\n",
    "2. **Soft Operations**: Use weighted combinations instead of discrete indexing\n",
    "3. **PyTorch Integration**: Implemented as `nn.Module` classes with learnable parameters\n",
    "4. **Gradient Flow**: All operations support backpropagation through autograd\n",
    "5. **Applications**: Can learn complex algorithms like list reversal end-to-end\n",
    "\n",
    "## Important Note on Gradient Flow\n",
    "\n",
    "**For demonstration purposes**, the `DifferentiableStack` class uses `.data` assignment which breaks gradient flow. This is fine for showing stack behavior but prevents learning.\n",
    "\n",
    "**For gradient-based optimization**, the `ListReverser` uses **functional operations** that maintain gradients:\n",
    "- No `.data` assignment \n",
    "- Returns new tensors instead of modifying in-place\n",
    "- Preserves computation graph for backpropagation\n",
    "\n",
    "The key insight is that by replacing discrete operations with their \"soft\" differentiable equivalents **while maintaining functional purity**, we can create data structures that neural networks can learn to manipulate programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
